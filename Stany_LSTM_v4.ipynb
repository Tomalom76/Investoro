{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03291e3b-0530-4afc-949b-214c8ae696ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ai\\AppData\\Local\\Temp\\ipykernel_12864\\3493048188.py:99: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  years = pd.to_datetime(full['BuiltYear'], errors='coerce').dt.year\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Balans klas przed treningiem ===\n",
      "                   count  percent\n",
      "BuildingCondition                \n",
      "AFTER_RENOVATION   22000    25.14\n",
      "DEVELOPER_STATE    22000    25.14\n",
      "FOR_RENOVATION     21513    24.58\n",
      "GOOD               22000    25.14\n",
      "\n",
      "Class weights (with GOOD boosted): {0: 0.9944602272727273, 1: 0.9944602272727273, 2: 1.0169959325973272, 3: 1.09390625}\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: konfiguracja, wczytanie i przygotowanie danych (jak w v2/v3 z drobnymi uzupełnieniami importów)\n",
    "\n",
    "import os, csv, io, re, json, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, f1_score  # <- dodano f1_score\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 4 źródła (każdy plik = jedna etykieta docelowa)\n",
    "FILES = {\n",
    "    'sf_after_renovation.csv': 'AFTER_RENOVATION',\n",
    "    'sf_developer_state.csv' : 'DEVELOPER_STATE',\n",
    "    'sf_for_renovation.csv'  : 'FOR_RENOVATION',\n",
    "    'sf_good.csv'            : 'GOOD',\n",
    "}\n",
    "\n",
    "# Wymagane przez model pola (zgodnie z notebookiem trenowania)\n",
    "REQUIRED_TEXT = ['Description']\n",
    "REQUIRED_NUM  = ['Area', 'Price', 'NumberOfRooms', 'Floor', 'Floors']\n",
    "REQUIRED_CAT  = ['BuildingType', 'OfferFrom', 'TypeOfMarket']\n",
    "REQUIRED_DT   = ['BuiltYear']  # źródło kolumny 'year'\n",
    "REQUIRED_ALL  = REQUIRED_TEXT + REQUIRED_NUM + REQUIRED_CAT + REQUIRED_DT\n",
    "\n",
    "# UWAGA: pliki źródłowe mają stałą kolejność kolumn w rekordach CSV.\n",
    "# Indeksy: 4=Description, 5=Area, 6=Price, 11=NumberOfRooms, 12=BuiltYear,\n",
    "# 14=BuildingType, 16=OfferFrom, 17=Floor, 18=Floors, 19=TypeOfMarket, 15=etykieta źródłowa\n",
    "IDX_MAP = {\n",
    "    'Description':    4,\n",
    "    'Area':           5,\n",
    "    'Price':          6,\n",
    "    'NumberOfRooms': 11,\n",
    "    'BuiltYear':     12,\n",
    "    'BuildingType':  14,\n",
    "    'OfferFrom':     16,\n",
    "    'Floor':         17,\n",
    "    'Floors':        18,\n",
    "    'TypeOfMarket':  19,\n",
    "}\n",
    "IDX_LABEL = 15\n",
    "\n",
    "def robust_read_records(path, encoding_candidates=('utf-8-sig','utf-8','cp1250','latin1')):\n",
    "    last_err = None\n",
    "    for enc in encoding_candidates:\n",
    "        try:\n",
    "            rows = []\n",
    "            with open(path, 'r', encoding=enc, errors='replace', newline='') as f:\n",
    "                reader = csv.reader(f, delimiter=',', quotechar='\"', escapechar='\\\\')\n",
    "                for row in reader:\n",
    "                    if not row or (len(row) == 1 and not str(row).strip()):\n",
    "                        continue\n",
    "                    rows.append(row)\n",
    "            if not rows:\n",
    "                continue\n",
    "            median_len = int(np.median([len(r) for r in rows]))\n",
    "            if median_len < 10:\n",
    "                continue\n",
    "            return rows\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "    if last_err:\n",
    "        raise last_err\n",
    "    raise RuntimeError(f'Unable to parse CSV: {path}')\n",
    "\n",
    "def extract_required_df(rows, force_label):\n",
    "    sel = {}\n",
    "    for name, idx in IDX_MAP.items():\n",
    "        sel[name] = [(r[idx] if len(r) > idx else None) for r in rows]\n",
    "    labels = [force_label for _ in rows]\n",
    "    out = pd.DataFrame(sel)\n",
    "    out['BuildingCondition'] = labels\n",
    "    return out\n",
    "\n",
    "frames = []\n",
    "for path, label in FILES.items():\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f'Brak pliku: {path}')\n",
    "    rows    = robust_read_records(path)\n",
    "    df_part = extract_required_df(rows, force_label=label)\n",
    "    frames.append(df_part)\n",
    "\n",
    "full = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# --- CZYSZCZENIE, TYPY, CECHY WTÓRNE ---\n",
    "\n",
    "full['Description'] = full['Description'].fillna('').astype(str)\n",
    "\n",
    "for col in ['Area','Price','NumberOfRooms','Floor','Floors']:\n",
    "    full[col] = pd.to_numeric(full[col], errors='coerce')\n",
    "\n",
    "years = pd.to_datetime(full['BuiltYear'], errors='coerce').dt.year\n",
    "if years.isna().all():\n",
    "    years = pd.to_numeric(full['BuiltYear'], errors='coerce')\n",
    "full['year'] = years\n",
    "full['year'] = full['year'].fillna(full['year'].median())\n",
    "\n",
    "for col in ['BuildingType','OfferFrom','TypeOfMarket']:\n",
    "    full[col] = full[col].fillna('unknown').astype(str).str.strip().str.lower()\n",
    "\n",
    "def norm_market(v):\n",
    "    v = (v or '').lower()\n",
    "    if 'pierwot' in v: return 'pierwotny'\n",
    "    if 'wtór' in v or 'wtorn' in v: return 'wtórny'\n",
    "    return v if v else 'unknown'\n",
    "full['TypeOfMarket'] = full['TypeOfMarket'].apply(norm_market)\n",
    "\n",
    "# --- BALANS KLAS (informacyjnie) ---\n",
    "counts  = full['BuildingCondition'].value_counts(dropna=False)\n",
    "perc    = (counts / len(full) * 100).round(2)\n",
    "balance = pd.DataFrame({'count': counts, 'percent': perc}).sort_index()\n",
    "print(\"\\n=== Balans klas przed treningiem ===\")\n",
    "print(balance)\n",
    "\n",
    "# --- PODZIAŁ I PRZYGOTOWANIE WEJŚĆ ---\n",
    "label_names  = ['AFTER_RENOVATION','DEVELOPER_STATE','FOR_RENOVATION','GOOD']\n",
    "label_to_idx = {name: i for i, name in enumerate(label_names)}\n",
    "y_idx = full['BuildingCondition'].map(label_to_idx).astype(int).values\n",
    "y     = to_categorical(y_idx, num_classes=len(label_names))\n",
    "\n",
    "max_words, max_len = 10000, 200\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"\")\n",
    "tokenizer.fit_on_texts(full['Description'].astype(str))\n",
    "X_text = pad_sequences(tokenizer.texts_to_sequences(full['Description'].astype(str)), maxlen=max_len)\n",
    "\n",
    "numeric_features     = ['Area', 'Price', 'NumberOfRooms', 'Floor', 'Floors', 'year']\n",
    "categorical_features = ['BuildingType', 'OfferFrom', 'TypeOfMarket']\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "X_tabular = preprocessor.fit_transform(full[numeric_features + categorical_features])\n",
    "\n",
    "X_text_train, X_text_test, X_tab_train, X_tab_test, y_train, y_test, y_idx_train, y_idx_test = train_test_split(\n",
    "    X_text, X_tabular, y, y_idx, test_size=0.2, random_state=42, stratify=y_idx\n",
    ")\n",
    "\n",
    "classes = np.unique(y_idx_train)\n",
    "cw = compute_class_weight(class_weight='balanced', classes=classes, y=y_idx_train)\n",
    "class_weight = {int(c): float(w) for c, w in zip(classes, cw)}\n",
    "\n",
    "# Delikatne doważenie klasy GOOD ~ +10%\n",
    "good_idx = label_to_idx['GOOD']\n",
    "class_weight[good_idx] = class_weight.get(good_idx, 1.0) * 1.10\n",
    "\n",
    "print('\\nClass weights (with GOOD boosted):', class_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6968586-6b00-4d21-8e79-9929dad2c010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ text_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │ text_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │          <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ tabular_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8220</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                 │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │         <span style=\"color: #00af00; text-decoration-color: #00af00\">263,072</span> │ tabular_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">6,208</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ text_input (\u001b[38;5;33mInputLayer\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)          │       \u001b[38;5;34m1,280,000\u001b[0m │ text_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │          \u001b[38;5;34m49,408\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ tabular_input (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8220\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                 │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │         \u001b[38;5;34m263,072\u001b[0m │ tabular_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │           \u001b[38;5;34m6,208\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                 │             \u001b[38;5;34m260\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,598,948</span> (6.10 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,598,948\u001b[0m (6.10 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,598,948</span> (6.10 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,598,948\u001b[0m (6.10 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trening: 20 epok, EarlyStopping/Checkpoint po val_macro_f1 + ReduceLROnPlateau po val_loss ...\n",
      "Epoch 1/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387ms/step - accuracy: 0.4494 - loss: 1.1947\n",
      "val_macro_f1: 0.5429\n",
      "\n",
      "Epoch 1: val_macro_f1 improved from -inf to 0.54293, saving model to model_best_macro_f1.keras\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 422ms/step - accuracy: 0.4495 - loss: 1.1946 - val_accuracy: 0.5614 - val_loss: 0.9671 - val_macro_f1: 0.5429 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 414ms/step - accuracy: 0.5611 - loss: 1.0268\n",
      "val_macro_f1: 0.6773\n",
      "\n",
      "Epoch 2: val_macro_f1 improved from 0.54293 to 0.67728, saving model to model_best_macro_f1.keras\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 448ms/step - accuracy: 0.5611 - loss: 1.0268 - val_accuracy: 0.6807 - val_loss: 0.8703 - val_macro_f1: 0.6773 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 430ms/step - accuracy: 0.6951 - loss: 0.8776\n",
      "val_macro_f1: 0.7210\n",
      "\n",
      "Epoch 3: val_macro_f1 improved from 0.67728 to 0.72101, saving model to model_best_macro_f1.keras\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 464ms/step - accuracy: 0.6951 - loss: 0.8775 - val_accuracy: 0.7195 - val_loss: 0.7803 - val_macro_f1: 0.7210 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 428ms/step - accuracy: 0.7310 - loss: 0.8048\n",
      "val_macro_f1: 0.7230\n",
      "\n",
      "Epoch 4: val_macro_f1 improved from 0.72101 to 0.72304, saving model to model_best_macro_f1.keras\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 461ms/step - accuracy: 0.7310 - loss: 0.8048 - val_accuracy: 0.7227 - val_loss: 0.7713 - val_macro_f1: 0.7230 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 447ms/step - accuracy: 0.6877 - loss: 0.8546\n",
      "val_macro_f1: 0.7415\n",
      "\n",
      "Epoch 5: val_macro_f1 improved from 0.72304 to 0.74146, saving model to model_best_macro_f1.keras\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 481ms/step - accuracy: 0.6877 - loss: 0.8546 - val_accuracy: 0.7382 - val_loss: 0.7340 - val_macro_f1: 0.7415 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 448ms/step - accuracy: 0.7520 - loss: 0.7520\n",
      "val_macro_f1: 0.7477\n",
      "\n",
      "Epoch 6: val_macro_f1 improved from 0.74146 to 0.74772, saving model to model_best_macro_f1.keras\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 482ms/step - accuracy: 0.7520 - loss: 0.7520 - val_accuracy: 0.7451 - val_loss: 0.7210 - val_macro_f1: 0.7477 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 441ms/step - accuracy: 0.7119 - loss: 0.8085\n",
      "val_macro_f1: 0.5336\n",
      "\n",
      "Epoch 7: val_macro_f1 did not improve from 0.74772\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 475ms/step - accuracy: 0.7118 - loss: 0.8087 - val_accuracy: 0.5632 - val_loss: 0.9551 - val_macro_f1: 0.5336 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 448ms/step - accuracy: 0.5876 - loss: 0.9566\n",
      "val_macro_f1: 0.5680\n",
      "\n",
      "Epoch 8: val_macro_f1 did not improve from 0.74772\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 485ms/step - accuracy: 0.5876 - loss: 0.9566 - val_accuracy: 0.5969 - val_loss: 0.9114 - val_macro_f1: 0.5680 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 451ms/step - accuracy: 0.6209 - loss: 0.8997\n",
      "val_macro_f1: 0.6003\n",
      "\n",
      "Epoch 9: val_macro_f1 did not improve from 0.74772\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 486ms/step - accuracy: 0.6209 - loss: 0.8997 - val_accuracy: 0.6129 - val_loss: 0.8832 - val_macro_f1: 0.6003 - learning_rate: 5.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 454ms/step - accuracy: 0.6425 - loss: 0.8696\n",
      "val_macro_f1: 0.6234\n",
      "\n",
      "Epoch 10: val_macro_f1 did not improve from 0.74772\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 489ms/step - accuracy: 0.6425 - loss: 0.8696 - val_accuracy: 0.6282 - val_loss: 0.8654 - val_macro_f1: 0.6234 - learning_rate: 5.0000e-04\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: architektura, kompilacja, makro-F1 callback i trening\n",
    "\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dropout, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "# Callback do obliczania macro-F1 na zbiorze walidacyjnym\n",
    "class ValMacroF1(Callback):\n",
    "    def __init__(self, val_data, batch_size=1024, verbose=1):\n",
    "        super().__init__()\n",
    "        self.X_text_val, self.X_tab_val, self.y_val = val_data\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        y_pred_proba = self.model.predict([self.X_text_val, self.X_tab_val],\n",
    "                                          batch_size=self.batch_size, verbose=0)\n",
    "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "        y_true = np.argmax(self.y_val, axis=1)\n",
    "        macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "        logs['val_macro_f1'] = macro_f1\n",
    "        # Zachowaj wynik w historii i wypisz\n",
    "        if self.verbose:\n",
    "            print(f\"\\nval_macro_f1: {macro_f1:.4f}\")\n",
    "\n",
    "# Architektura (jak w v2/v3)\n",
    "text_input     = Input(shape=(max_len,), name='text_input')\n",
    "embedding_layer= Embedding(input_dim=max_words, output_dim=128)(text_input)\n",
    "lstm_layer     = LSTM(64, recurrent_dropout=0.2)(embedding_layer)\n",
    "dropout_lstm   = Dropout(0.4)(lstm_layer)\n",
    "\n",
    "tabular_input  = Input(shape=(X_tab_train.shape[1],), name='tabular_input')\n",
    "tabular_dense  = Dense(32, activation='relu')(tabular_input)\n",
    "\n",
    "concatenated   = Concatenate()([dropout_lstm, tabular_dense])\n",
    "dense1         = Dense(64, activation='relu')(concatenated)\n",
    "dropout_final  = Dropout(0.5)(dense1)\n",
    "output         = Dense(len(label_names), activation='softmax')(dropout_final)\n",
    "\n",
    "model = Model(inputs=[text_input, tabular_input], outputs=output)\n",
    "model.summary()\n",
    "\n",
    "# Kompilacja: label_smoothing obniżone do 0.02\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    loss=CategoricalCrossentropy(label_smoothing=0.02),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacki: najpierw makro-F1 (aby EarlyStopping/Checkpoint widziały metrykę w logs)\n",
    "macro_cb = ValMacroF1(val_data=(X_text_test, X_tab_test, y_test), batch_size=1024, verbose=1)\n",
    "\n",
    "es  = EarlyStopping(monitor='val_macro_f1', mode='max', patience=4,\n",
    "                    restore_best_weights=True, verbose=1)\n",
    "ckp = ModelCheckpoint('model_best_macro_f1.keras', monitor='val_macro_f1', mode='max',\n",
    "                      save_best_only=True, verbose=1)\n",
    "# Redukcja LR nadal po val_loss (stabilny sygnał dla optymalizatora)\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2,\n",
    "                        min_lr=1e-5, verbose=1)\n",
    "\n",
    "print(\"\\nTrening: 20 epok, EarlyStopping/Checkpoint po val_macro_f1 + ReduceLROnPlateau po val_loss ...\")\n",
    "history = model.fit(\n",
    "    [X_text_train, X_tab_train], y_train,\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    validation_data=([X_text_test, X_tab_test], y_test),\n",
    "    callbacks=[macro_cb, es, ckp, rlr],\n",
    "    class_weight=class_weight\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9ce6315-374f-45a5-82b4-0575b0abc302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 84ms/step - accuracy: 0.7456 - loss: 0.7215\n",
      "\n",
      "Dokładność na zbiorze testowym: 0.7451\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 72ms/step\n",
      "\n",
      "Raport klasyfikacji na zbiorze testowym:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "AFTER_RENOVATION       0.61      0.75      0.68      4400\n",
      " DEVELOPER_STATE       0.93      0.89      0.91      4400\n",
      "  FOR_RENOVATION       0.83      0.71      0.77      4303\n",
      "            GOOD       0.65      0.62      0.64      4400\n",
      "\n",
      "        accuracy                           0.75     17503\n",
      "       macro avg       0.76      0.74      0.75     17503\n",
      "    weighted avg       0.76      0.75      0.75     17503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: ocena na teście (accuracy i raport per klasa)\n",
    "\n",
    "loss, accuracy = model.evaluate([X_text_test, X_tab_test], y_test)\n",
    "print(f\"\\nDokładność na zbiorze testowym: {accuracy:.4f}\")\n",
    "\n",
    "y_pred_proba = model.predict([X_text_test, X_tab_test])\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"\\nRaport klasyfikacji na zbiorze testowym:\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af80b7e0-fac5-447a-a8dc-03ffb4af6f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zapisano: model_lstm_stan.keras, tokenizer.json, preprocessor.joblib, label_mapping.json, columns_for_prediction.joblib\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: zapis artefaktów\n",
    "\n",
    "import joblib, json\n",
    "\n",
    "# Zapis aktualnych wag (po EarlyStopping) oraz preprocesora i tokenizera\n",
    "model.save('model_lstm_stan.keras')\n",
    "\n",
    "with open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer.to_json(), ensure_ascii=False))\n",
    "\n",
    "joblib.dump(preprocessor, 'preprocessor.joblib')\n",
    "\n",
    "label_mapping = {i: name for i, name in enumerate(label_names)}\n",
    "with open('label_mapping.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump({str(k): v for k, v in label_mapping.items()}, f, ensure_ascii=False)\n",
    "\n",
    "columns_for_prediction = numeric_features + categorical_features\n",
    "joblib.dump(columns_for_prediction, 'columns_for_prediction.joblib')\n",
    "\n",
    "print(\"Zapisano: model_lstm_stan.keras, tokenizer.json, preprocessor.joblib, label_mapping.json, columns_for_prediction.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4e0c22-6229-493b-8702-5075238c9c33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
