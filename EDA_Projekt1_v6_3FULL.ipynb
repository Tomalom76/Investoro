{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d60d7850-65ad-49cc-899b-d2f8d0ef2dbd",
   "metadata": {},
   "source": [
    "# PROJEKT MODELU REGRESYJNEGO PRZEWIDYWANIA CEN MIESZKAŃ NA PODSTAWIE #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6227290f-4c53-4631-87c2-0b50214059e9",
   "metadata": {},
   "source": [
    "# ALGORYTMU LGBM #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6a9457-72c0-4a42-994b-3764bad0b8b4",
   "metadata": {},
   "source": [
    "  ## KWIECIEŃ 2025 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cc381d5-6d67-40d8-8bb8-82ffb690bb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "# from pycaret.datasets import get_data \n",
    "from pycaret.regression import setup, pull, compare_models, plot_model, load_model, tune_model, finalize_model, save_model, predict_model, get_config\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np\n",
    "# from scipy.stats import skewnorm \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from joblib import parallel_backend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a6099c-45af-433a-ba42-9707c7857c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_EXPERIMENT_NAME = 'Investoro_Ceny'\n",
    "MLFLOW_TAGS = {'data': 'Investoro_ceny', 'library': 'pycaret'}\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a3986-18f7-41af-af75-f55a2eb603db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tę komórkę uruchom jeśli czerpiesz dane z pliku .csv\n",
    "df_original  = pd.read_csv('data.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a438f6-6101-4fa6-8e94-57f58b77126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb387ca-0a12-465b-b2df-910fdab53ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original .head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55273e7-7e8a-4c0c-8fc3-157666a44383",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original .sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1793515a-e3f6-4cf5-81c2-4f7110e0239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcea7f92-f9bc-49a8-85bd-f60d39b358d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original [df_original .duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad96f83-0401-451c-89b8-117e838c237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original .nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25c261d-c45b-44f1-bb76-f7fc6dee01fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_temp = df_original.copy()\n",
    "if pd.api.types.is_string_dtype(df_corr_temp['BuiltYear']):\n",
    "    df_corr_temp['BuiltYear_Num'] = pd.to_datetime(df_corr_temp['BuiltYear'], format='%Y', errors='coerce').dt.year\n",
    "elif pd.api.types.is_datetime64_any_dtype(df_corr_temp['BuiltYear']):\n",
    "     df_corr_temp['BuiltYear_Num'] = df_corr_temp['BuiltYear'].dt.year\n",
    "else:\n",
    "    df_corr_temp['BuiltYear_Num'] = pd.to_numeric(df_corr_temp['BuiltYear'], errors='coerce') # Ostateczna próba\n",
    "\n",
    "cols_for_corr = ['Area', 'Price', 'BuiltYear_Num', 'Floor', 'Floors', 'CommunityScore', 'CountyNumber', 'CommunityNumber',\n",
    "                   'RegionNumber','KindNumber']\n",
    "# Upewnij się, że wszystkie kolumny istnieją i są numeryczne\n",
    "valid_cols_for_corr = [col for col in cols_for_corr if col in df_corr_temp.columns and pd.api.types.is_numeric_dtype(df_corr_temp[col])]\n",
    "correlation_matrix = df_corr_temp[valid_cols_for_corr].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647398a1-f587-4525-ba95-aa65882231b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix of Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd8578b-2b18-4a65-8d6c-7d5e2b2beab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original .isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688cc921-92bd-42ed-a19d-ebed090b7eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original .isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0861e61c-360f-4f8f-9760-962e8f40263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_original.copy()\n",
    "print(f\"Rozmiar df_cleaned przed czyszczeniem: {df_cleaned.shape}\")\n",
    "\n",
    "df_cleaned.dropna(subset=['Area', 'Price', 'Location'], inplace=True)\n",
    "print(f\"Rozmiar df_cleaned po usunięciu NaN z Area, Price, Location: {df_cleaned.shape}\")\n",
    "display(df_cleaned.isnull().sum().sort_values(ascending=False).head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc55c11f-9bf2-49f9-ad3d-abb86c1366a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_price = df_cleaned[\"Price\"].quantile(0.25)\n",
    "Q3_price = df_cleaned[\"Price\"].quantile(0.75)\n",
    "IQR_price = Q3_price - Q1_price\n",
    "lower_bound_price = Q1_price - 1.5 * IQR_price\n",
    "upper_bound_price = Q3_price + 1.5 * IQR_price\n",
    "df_cleaned = df_cleaned[~((df_cleaned[\"Price\"] < lower_bound_price) | (df_cleaned[\"Price\"] > upper_bound_price))]\n",
    "print(f\"Rozmiar df_cleaned po usunięciu outlierów z Price: {df_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d7b8eb-214b-47ee-98e0-09879b533e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"PricePerSquareMeter\" in df_cleaned.columns and df_cleaned[\"PricePerSquareMeter\"].isnull().sum() < len(df_cleaned) * 0.8: \n",
    "    df_cleaned.dropna(subset=['PricePerSquareMeter'], inplace=True) \n",
    "    Q1_ppsm = df_cleaned[\"PricePerSquareMeter\"].quantile(0.25)\n",
    "    Q3_ppsm = df_cleaned[\"PricePerSquareMeter\"].quantile(0.75)\n",
    "    IQR_ppsm = Q3_ppsm - Q1_ppsm\n",
    "    lower_bound_ppsm = Q1_ppsm - 1.5 * IQR_ppsm\n",
    "    upper_bound_ppsm = Q3_ppsm + 1.5 * IQR_ppsm\n",
    "    df_cleaned = df_cleaned[~((df_cleaned[\"PricePerSquareMeter\"] < lower_bound_ppsm) | (df_cleaned[\"PricePerSquareMeter\"] > upper_bound_ppsm))]\n",
    "    print(f\"Rozmiar df_cleaned po usunięciu outlierów z PricePerSquareMeter: {df_cleaned.shape}\")\n",
    "else:\n",
    "    print(\"Kolumna 'PricePerSquareMeter' nie użyta do usuwania outlierów (brak lub za dużo NaN).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bb715e-9045-4468-839d-04570d568cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_area = df_cleaned[\"Area\"].quantile(0.25)\n",
    "Q3_area = df_cleaned[\"Area\"].quantile(0.75)\n",
    "IQR_area = Q3_area - Q1_area\n",
    "lower_bound_area = Q1_area - 1.5 * IQR_area\n",
    "upper_bound_area = Q3_area + 1.5 * IQR_area\n",
    "df_cleaned = df_cleaned[~((df_cleaned[\"Area\"] < lower_bound_area) | (df_cleaned[\"Area\"] > upper_bound_area))]\n",
    "print(f\"Rozmiar df_cleaned po usunięciu outlierów z Area: {df_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4d556c-b787-48c6-8de0-3d298e07a83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['BuiltYear'] = pd.to_datetime(df_cleaned['BuiltYear'], format='%Y', errors='coerce')\n",
    "print(\"Konwersja BuiltYear na datetime w df_cleaned zakończona.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ff33da-1702-43ca-bccb-839f48bf7f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Informacje o df_cleaned po wszystkich krokach czyszczenia:\")\n",
    "df_cleaned.info()\n",
    "print(\"\\nBraki danych w df_cleaned (%):\")\n",
    "display(df_cleaned.isnull().sum() / len(df_cleaned) * 100)\n",
    "print(\"\\nPierwsze wiersze df_cleaned:\")\n",
    "display(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ef8a46-c455-41ad-9813-75d4ba30ea96",
   "metadata": {},
   "source": [
    "# Sprawdzenie braków - procentowo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869c2b5e-56ff-4f3f-9bbe-9c673566fd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Rozmiar df_cleaned przed podziałem na train/holdout: {df_cleaned.shape}\")\n",
    "train_df = df_cleaned.sample(frac=0.9, random_state=42)\n",
    "holdout_df = df_cleaned.drop(train_df.index)\n",
    "\n",
    "print(f\"Rozmiar zbioru treningowego (train_df): {train_df.shape}\")\n",
    "print(f\"Rozmiar zbioru holdout (holdout_df): {holdout_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405f786b-428f-43df-94ed-db39cd2d936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_column_types(df_to_convert):\n",
    "    df_copy = df_to_convert.copy()\n",
    "    str_cols = ['VoivodeshipNumber', 'CountyNumber', 'CommunityNumber', 'KindNumber', 'RegionNumber', 'StreetNumber'] # Dodano StreetNumber\n",
    "    for col in str_cols:\n",
    "        if col in df_copy.columns:\n",
    "            df_copy[col] = df_copy[col].astype(str)\n",
    "    \n",
    "    # BuiltYear powinno być już datetime, ale upewnijmy się\n",
    "    if 'BuiltYear' in df_copy.columns and not pd.api.types.is_datetime64_any_dtype(df_copy['BuiltYear']):\n",
    "         df_copy['BuiltYear'] = pd.to_datetime(df_copy['BuiltYear'], format='%Y', errors='coerce')\n",
    "    return df_copy\n",
    "\n",
    "train_df = convert_column_types(train_df)\n",
    "holdout_df = convert_column_types(holdout_df)\n",
    "\n",
    "print(\"\\\\nTypy danych w train_df po konwersji:\")\n",
    "train_df.info()\n",
    "print(\"\\\\nTypy danych w holdout_df po konwersji:\")\n",
    "holdout_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c638eb3c-0c67-4f4e-a642-ea30a3d3c907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "location_vectorizer = TfidfVectorizer(\n",
    "    max_features=100, \n",
    "    stop_words=None,\n",
    "    ngram_range=(1, 1),\n",
    "    min_df=5,\n",
    "    max_df=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b913ec27-396c-4749-8a46-375e005598d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Przetwarzanie TF-IDF dla zbioru treningowego...\")\n",
    "train_df_copy = train_df.copy() # Pracujemy na kopii\n",
    "train_df_copy['Location_Clean'] = train_df_copy['Location'].fillna('').astype(str)\n",
    "train_location_tfidf_features = location_vectorizer.fit_transform(train_df_copy['Location_Clean'])\n",
    "\n",
    "try:\n",
    "    feature_names = location_vectorizer.get_feature_names_out()\n",
    "except AttributeError:\n",
    "    feature_names = location_vectorizer.get_feature_names_() \n",
    "    \n",
    "train_location_tfidf_df = pd.DataFrame(\n",
    "    train_location_tfidf_features.toarray(),\n",
    "    columns=['loc_tfidf_' + name for name in feature_names],\n",
    "    index=train_df_copy.index # Ważne, aby zachować oryginalny indeks\n",
    ")\n",
    "print(f\"Utworzono {train_location_tfidf_df.shape[1]} cech TF-IDF dla zbioru treningowego.\")\n",
    "\n",
    "train_df_processed = pd.concat(\n",
    "    [train_df_copy.drop(columns=['Location', 'Location_Clean'], errors='ignore'), train_location_tfidf_df], \n",
    "    axis=1\n",
    ")\n",
    "# WAŻNE: Usuń wiersze gdzie 'Price' < 20000 (lub inna wartość) dopiero PO przetworzeniu TF-IDF, \n",
    "# aby uniknąć problemów z niedopasowaniem indeksów przy konkatenacji.\n",
    "train_df_processed = train_df_processed[train_df_processed['Price'] >= 20000] \n",
    "print(f\"Rozmiar train_df_processed po usunięciu cen < 20000: {train_df_processed.shape}\")\n",
    "display(train_df_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a9726-a795-4a19-a1c5-6282f1afba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Przetwarzanie TF-IDF dla zbioru holdout...\")\n",
    "holdout_df_copy = holdout_df.copy() # Pracujemy na kopii\n",
    "holdout_df_copy['Location_Clean'] = holdout_df_copy['Location'].fillna('').astype(str)\n",
    "holdout_location_tfidf_features = location_vectorizer.transform(holdout_df_copy['Location_Clean']) # Użyj transform\n",
    "\n",
    "holdout_location_tfidf_df = pd.DataFrame(\n",
    "    holdout_location_tfidf_features.toarray(),\n",
    "    columns=['loc_tfidf_' + name for name in feature_names],\n",
    "    index=holdout_df_copy.index # Ważne, aby zachować oryginalny indeks\n",
    ")\n",
    "print(f\"Utworzono {holdout_location_tfidf_df.shape[1]} cech TF-IDF dla zbioru holdout.\")\n",
    "\n",
    "holdout_df_processed = pd.concat(\n",
    "    [holdout_df_copy.drop(columns=['Location', 'Location_Clean'], errors='ignore'), holdout_location_tfidf_df],\n",
    "    axis=1\n",
    ")\n",
    "print(f\"Rozmiar holdout_df_processed: {holdout_df_processed.shape}\")\n",
    "display(holdout_df_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41d1226-eb6a-4425-bdce-3229b0d639ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features_initial = [\n",
    "    'BuildingType', 'BuildingCondition', 'TypeOfMarket', 'OwnerType', 'Type', 'OfferFrom',\n",
    "    'VoivodeshipNumber', 'CountyNumber', 'CommunityNumber', 'KindNumber', 'RegionNumber'\n",
    "]\n",
    "numeric_features_initial = [\n",
    "    'Area', 'NumberOfRooms', 'Floor', 'Floors', 'CommunityScore'\n",
    "]\n",
    "date_features_initial = ['BuiltYear']\n",
    "\n",
    "categorical_features_to_use = [col for col in categorical_features_initial if col in train_df_processed.columns]\n",
    "numeric_features_to_use = [col for col in numeric_features_initial if col in train_df_processed.columns]\n",
    "# Kolumny loc_tfidf_* zostaną dodane do numeric_features wewnątrz setup\n",
    "date_features_to_use = [col for col in date_features_initial if col in train_df_processed.columns]\n",
    "\n",
    "ignore_features_list_setup = [ # (...) lista jak w oryginalnym kodzie\n",
    "    'SaleId', 'OriginalId', 'PortalId', 'Title', 'Description',\n",
    "    'OfferPrice', 'RealPriceAfterRenovation', 'OriginalPrice',\n",
    "    'PricePerSquareMeter', 'DateAddedToDatabase', 'DateAdded',\n",
    "    'DateLastModification', 'DateLastRaises', 'NewestDate',\n",
    "    'AvailableFrom', 'Link', 'Phone', 'MainImage', 'OtherImages',\n",
    "    'NumberOfDuplicates', 'NumberOfRaises', 'NumberOfModifications',\n",
    "    'IsDuplicatePriceLower', 'IsDuplicatePrivateOwner', 'Score', 'ScorePrecision',\n",
    "    'NumberOfCommunityComments', 'NumberOfCommunityOpinions', 'Archive',\n",
    "    'SubRegionNumber', 'EncryptedId',\n",
    "    'StreetNumber' # StreetNumber jest teraz stringiem, jeśli ma za dużo wartości, można go tu dodać\n",
    "]\n",
    "ignore_features_final = [col for col in ignore_features_list_setup if col in train_df_processed.columns]\n",
    "\n",
    "print(\"--- Informacje przed PyCaret setup ---\")\n",
    "print(\"Liczba kolumn w train_df_processed:\", len(train_df_processed.columns.tolist()))\n",
    "print(\"Cechy kategoryczne:\", categorical_features_to_use)\n",
    "print(\"Cechy numeryczne (początkowe):\", numeric_features_to_use)\n",
    "print(\"Cechy daty:\", date_features_to_use)\n",
    "print(\"Ignorowane cechy:\", ignore_features_final)\n",
    "print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68a603e-d67f-48a6-8ffd-5d0f6b0ab7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Utwórz dedykowany katalog dla tego testu, jeśli nie istnieje\n",
    "current_directory = os.getcwd() \n",
    "local_mlruns_path = os.path.join(current_directory, \"mlruns_DIRECT_LOCAL_TEST\") \n",
    "\n",
    "if not os.path.exists(local_mlruns_path):\n",
    "    os.makedirs(local_mlruns_path)\n",
    "    print(f\"Utworzono katalog: {local_mlruns_path}\")\n",
    "else:\n",
    "    print(f\"Katalog już istnieje: {local_mlruns_path}\")\n",
    "\n",
    "absolute_mlruns_path = os.path.abspath(local_mlruns_path)\n",
    "tracking_uri = f\"file:///{absolute_mlruns_path.replace(os.sep, '/')}\"\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "\n",
    "print(f\"MLflow tracking URI ustawione na: {mlflow.get_tracking_uri()}\")\n",
    "\n",
    "# MLFLOW_EXPERIMENT_NAME powinno być zdefiniowane wcześniej w Twoim notebooku\n",
    "# np. MLFLOW_EXPERIMENT_NAME = 'Investoro_Ceny'\n",
    "\n",
    "try:\n",
    "    # Sprawdź, czy eksperyment istnieje\n",
    "    experiment = mlflow.get_experiment_by_name(MLFLOW_EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        # Jeśli nie istnieje, stwórz go.\n",
    "        # Dla logowania typu 'file://', MLflow sam zarządzi lokalizacją artefaktów\n",
    "        # w podkatalogach struktury 'mlruns'.\n",
    "        experiment_id = mlflow.create_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "        print(f\"Utworzono nowy eksperyment MLflow: '{MLFLOW_EXPERIMENT_NAME}' o ID: {experiment_id}\")\n",
    "    else:\n",
    "        experiment_id = experiment.experiment_id\n",
    "        print(f\"Znaleziono istniejący eksperyment: '{MLFLOW_EXPERIMENT_NAME}' o ID: {experiment_id}\")\n",
    "    \n",
    "    # Ustaw eksperyment jako aktywny\n",
    "    mlflow.set_experiment(experiment_name=MLFLOW_EXPERIMENT_NAME)\n",
    "    print(f\"Aktywny eksperyment MLflow ustawiony na: '{MLFLOW_EXPERIMENT_NAME}'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Błąd podczas ustawiania/tworzenia eksperymentu MLflow: {e}\")\n",
    "    import traceback\n",
    "    print(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5611682d-06d6-4193-a7b1-62b7b49da576",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_exp = None \n",
    "try:\n",
    "    loc_tfidf_cols = [col for col in train_df_processed.columns if 'loc_tfidf_' in col]\n",
    "    reg_exp = setup(\n",
    "        data=train_df_processed, \n",
    "        target='Price',\n",
    "        log_experiment=True,\n",
    "        experiment_name=MLFLOW_EXPERIMENT_NAME,\n",
    "        categorical_features=categorical_features_to_use,\n",
    "        numeric_features=numeric_features_to_use + loc_tfidf_cols, # Dodajemy cechy TF-IDF\n",
    "        date_features=date_features_to_use,\n",
    "        ignore_features=ignore_features_final,\n",
    "        # ... (reszta parametrów jak w oryginalnym kodzie) ...\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Błąd podczas setup PyCaret: {e}\")\n",
    "    import traceback\n",
    "    print(traceback.format_exc())\n",
    "\n",
    "if reg_exp:\n",
    "    best_model_from_compare = compare_models() # Zapisz najlepszy model z compare_models\n",
    "    compare_metrics_df = pull()\n",
    "    display(compare_metrics_df)\n",
    "else:\n",
    "    print(\"Nie udało się zainicjować eksperymentu PyCaret.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c34157-105f-475b-8632-168d899b6ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_exp.X_train_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e9266-8dbf-415e-898f-b787241c9057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.regression import get_config\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Wyciągnij dane oryginalne i przetransformowane\n",
    "df_raw = train_df_processed.copy()\n",
    "df_transformed = get_config(\"X_train\").copy()\n",
    "df_transformed[\"Price\"] = get_config(\"y_train\")\n",
    "\n",
    "# Rysowanie wykresów\n",
    "#fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "#sns.histplot(df_raw[\"Price\"], ax=axes[0])\n",
    "#axes[0].set_title(\"Raw Data\")\n",
    "\n",
    "#sns.histplot(df_transformed[\"Price\"], ax=axes[1])\n",
    "#axes[1].set_title(\"Transformed Data\")\n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80462c2-5d5c-4523-9f20-634c227a6240",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_exp.dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd74d03-a7c5-4c9c-ad6a-1c9d77554ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_exp.dataset_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb1bb2b-ead0-4d05-aa01-08b58a74c761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_transformed.plot.scatter(x='Area', y='Price');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7bc941-ec3e-4c43-8b8d-582a4987eef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_transformed.plot.scatter(x='BuiltYear', y='Price');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e618c478-53c5-4a12-9135-d94ffc936183",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_exp.plot_model(best_model_from_compare, plot='feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a77363-b384-49e7-be2f-f184e18b096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reg_exp and best_model_from_compare:\n",
    "    print(\"Predykcja na zbiorze testowym (z podziału PyCaret) przy użyciu dostrojonego modelu:\")\n",
    "    predict_model(best_model_from_compare) # To wyświetli metryki na wewnętrznym zbiorze testowym\n",
    "    test_set_metrics_after_tuning = pull()\n",
    "    display(test_set_metrics_after_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81108f4-b6e5-41f0-8e5f-497bb971dcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_final_model = None\n",
    "if reg_exp and best_model_from_compare:\n",
    "    best_final_model = finalize_model(best_model_from_compare, experiment_custom_tags={\"step\": \"final_tuned\"})\n",
    "    print(\"Sfinalizowany model (po strojeniuu):\")\n",
    "    display(best_final_model)\n",
    "elif reg_exp and 'best_model_from_compare' in locals() and best_model_from_compare is not None:\n",
    "    print(\"Strojenie nie powiodło się lub zostało pominięte. Finalizuję najlepszy model z compare_models.\")\n",
    "    best_final_model = finalize_model(best_model_from_compare, experiment_custom_tags={\"step\": \"final_compare\"})\n",
    "    display(best_final_model)\n",
    "else:\n",
    "    print(\"Nie można sfinalizować modelu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0701e4-a5ba-4392-9229-2f5db3a0f2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(best_final_model, '0_full-basic-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6a42c5-14b2-4d48-9647-2e23d00d0d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_model(best_final_model, data=holdout_df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc66173-b904-4546-be80-fa8a103e4a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c89ec1-e90a-4c79-b6d9-6b8b0b058c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_holdout_predictions_df = None # Zmiana nazwy dla spójności ze skryptem\n",
    "if reg_exp and best_final_model and 'holdout_df_processed' in locals() and not holdout_df_processed.empty:\n",
    "    print(\"Predykcja na zbiorze holdout (za pomocą sfinalizowanego modelu)...\")\n",
    "    holdout_data_for_pred = holdout_df_processed.copy()\n",
    "    \n",
    "    # Dodanie prawdziwych cen do DataFrame z predykcjami dla łatwiejszej oceny (jeśli są dostępne)\n",
    "    if 'Price' in holdout_data_for_pred.columns:\n",
    "        holdout_data_for_pred = holdout_data_for_pred.drop(columns=['Price'])\n",
    "\n",
    "    final_holdout_predictions_df = predict_model(best_final_model, data=holdout_data_for_pred)    \n",
    "\n",
    "        # Można tu obliczyć metryki ręcznie lub użyć narzędzi sklearn\n",
    "        from sklearn.metrics import r2_score, mean_absolute_error\n",
    "        r2 = r2_score(final_holdout_predictions_df['Actual_Price'], final_holdout_predictions_df['prediction_label'])\n",
    "        mae = mean_absolute_error(final_holdout_predictions_df['Actual_Price'], final_holdout_predictions_df['prediction_label'])\n",
    "        print(f\"Metryki na zbiorze holdout (sfinalizowany model): R2 = {r2:.4f}, MAE = {mae:.2f}\")\n",
    "        if mlflow.active_run():\n",
    "             mlflow.log_metric(\"holdout_final_R2\", r2)\n",
    "             mlflow.log_metric(\"holdout_final_MAE\", mae)\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Nie można wykonać predykcji na zbiorze holdout - brak sfinalizowanego modelu lub danych.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc0105d-8b60-4d2b-b368-3466c2621b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "if final_holdout_predictions_df is not None and 'holdout_df' in locals():\n",
    "    # holdout_df to oryginalny zbiór holdout PRZED dodaniem cech TF-IDF i innymi transformacjami PyCaret\n",
    "    # final_holdout_predictions_df ma predykcje i oryginalny indeks z holdout_df_processed\n",
    "\n",
    "    # Przygotuj df_last: SaleId, oryginalna cena (Price), przewidziana cena (prediction_label)\n",
    "    df_last = holdout_df[['SaleId', 'Price']].copy() # Używamy holdout_df dla oryginalnych wartości\n",
    "    \n",
    "    # Dodajemy kolumnę predykcji. Musimy upewnić się, że indeksy pasują.\n",
    "    # predict_model zachowuje indeks oryginalnego DataFrame przekazanego do `data=`\n",
    "    df_last['PredictedPrice_num'] = final_holdout_predictions_df.loc[df_last.index, 'prediction_label']\n",
    "\n",
    "    # Formatowanie dla czytelności w CSV\n",
    "    df_last['Price_formatted'] = df_last['Price'].apply(lambda x: f\"{x:,.0f}\" if pd.notnull(x) else None)\n",
    "    df_last['PredictedPrice_formatted'] = df_last['PredictedPrice_num'].apply(lambda x: f\"{x:,.0f}\" if pd.notnull(x) else None)\n",
    "    \n",
    "    # Zapisz plik porównawczy z oryginalną ceną i przewidywaną\n",
    "    df_last_to_save_compare = df_last[['SaleId', 'Price_formatted', 'PredictedPrice_formatted']].rename(\n",
    "        columns={'Price_formatted': 'Original_Price', 'PredictedPrice_formatted': 'Predicted_Price'}\n",
    "    )\n",
    "    df_last_to_save_compare.to_csv('0_new_prices_compare.csv', index=False)\n",
    "    print(\"Plik 0_new_prices_compare.csv został zapisany.\")\n",
    "    display(df_last_to_save_compare.head())\n",
    "\n",
    "    # Zapisz pełny oryginalny zbiór holdout z dodaną kolumną predykcji\n",
    "    holdout_df_with_predictions = holdout_df.copy()\n",
    "    # Ponownie, dopasuj po indeksie\n",
    "    holdout_df_with_predictions['PredictedPrice'] = final_holdout_predictions_df.loc[holdout_df_with_predictions.index, 'prediction_label']\n",
    "    \n",
    "    cols_to_save = list(holdout_df.columns) # Oryginalne kolumny\n",
    "    cols_to_save.insert(cols_to_save.index('Price') + 1, 'PredictedPrice') # Wstaw PredictedPrice po Price\n",
    "    # Usuń PredictedPrice z końca, jeśli już tam jest przez przypadek\n",
    "    if 'PredictedPrice' in cols_to_save[:-1] and cols_to_save[-1] == 'PredictedPrice':\n",
    "         cols_to_save.pop()\n",
    "    \n",
    "    holdout_df_with_predictions = holdout_df_with_predictions[cols_to_save]\n",
    "\n",
    "    holdout_df_with_predictions.to_csv('full_holdout_with_predictions.csv', index=False)\n",
    "    print(\"Plik full_holdout_with_predictions.csv został zapisany.\")\n",
    "    display(holdout_df_with_predictions.head())\n",
    "\n",
    "else:\n",
    "    print(\"Nie można zapisać wyników - brak predykcji (final_holdout_predictions_df) lub oryginalnego zbioru holdout (holdout_df).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8279d95e-4d6c-4cba-bf14-6b658ca8d0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Komórka do przygotowania df_original_for_prediction\n",
    "print(\"Przygotowywanie df_original do predykcji...\")\n",
    "\n",
    "df_original_for_prediction = df_original.copy()\n",
    "\n",
    "df_original_for_prediction.dropna(subset=['Area', 'Location'], inplace=True)\n",
    "print(f\"Rozmiar po usunięciu NaN z Area, Location: {df_original_for_prediction.shape}\")\n",
    "\n",
    "\n",
    "if not df_original_for_prediction.empty: # Sprawdzenie czy DataFrame nie jest pusty\n",
    "    Q1_area_orig = df_original_for_prediction[\"Area\"].quantile(0.25)\n",
    "    Q3_area_orig = df_original_for_prediction[\"Area\"].quantile(0.75)\n",
    "    IQR_area_orig = Q3_area_orig - Q1_area_orig\n",
    "    lower_bound_area_orig = Q1_area_orig - 1.5 * IQR_area_orig\n",
    "    upper_bound_area_orig = Q3_area_orig + 1.5 * IQR_area_orig\n",
    "    df_original_for_prediction = df_original_for_prediction[\n",
    "        ~((df_original_for_prediction[\"Area\"] < lower_bound_area_orig) | (df_original_for_prediction[\"Area\"] > upper_bound_area_orig))\n",
    "    ]\n",
    "    print(f\"Rozmiar po usunięciu outlierów z Area: {df_original_for_prediction.shape}\")\n",
    "else:\n",
    "    print(\"DataFrame jest pusty po usunięciu NaN, pomijam usuwanie outlierów.\")\n",
    "\n",
    "if not df_original_for_prediction.empty:\n",
    "    df_original_for_prediction = convert_column_types(df_original_for_prediction) # Używamy funkcji convert_column_types\n",
    "    print(\"Konwersja typów danych w df_original_for_prediction zakończona.\")\n",
    "else:\n",
    "    print(\"DataFrame jest pusty, pomijam konwersję typów.\")\n",
    "\n",
    "\n",
    "if not df_original_for_prediction.empty:\n",
    "    df_original_for_prediction['Location_Clean'] = df_original_for_prediction['Location'].fillna('').astype(str)\n",
    "    original_location_tfidf_features = location_vectorizer.transform(df_original_for_prediction['Location_Clean'])\n",
    "\n",
    "    try:\n",
    "        feature_names_loc = location_vectorizer.get_feature_names_out()\n",
    "    except AttributeError:\n",
    "        feature_names_loc = location_vectorizer.get_feature_names_()\n",
    "\n",
    "    original_location_tfidf_df = pd.DataFrame(\n",
    "        original_location_tfidf_features.toarray(),\n",
    "        columns=['loc_tfidf_' + name for name in feature_names_loc],\n",
    "        index=df_original_for_prediction.index\n",
    "    )\n",
    "\n",
    "    df_original_processed = pd.concat(\n",
    "        [df_original_for_prediction.drop(columns=['Location', 'Location_Clean'], errors='ignore'), original_location_tfidf_df],\n",
    "        axis=1\n",
    "    )\n",
    "    print(f\"Utworzono {original_location_tfidf_df.shape[1]} cech TF-IDF dla df_original_processed.\")\n",
    "    print(f\"Rozmiar df_original_processed: {df_original_processed.shape}\")\n",
    "else:\n",
    "    print(\"DataFrame jest pusty, pomijam przetwarzanie TF-IDF.\")\n",
    "    df_original_processed = pd.DataFrame() # Pusty DataFrame, aby uniknąć błędów później\n",
    "\n",
    "display(df_original_processed.head() if not df_original_processed.empty else \"DataFrame jest pusty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006ac2b9-08df-474a-9eb4-14d31126aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_predictions = None # Inicjalizacja\n",
    "\n",
    "if best_final_model and not df_original_processed.empty:\n",
    "    print(f\"Rozpoczynanie predykcji na df_original_processed o kształcie: {df_original_processed.shape}\")\n",
    "    \n",
    "    data_for_prediction_final = df_original_processed.copy()\n",
    "    \n",
    "    # ZAPAMIĘTAJ oryginalne ceny, jeśli istnieją, zanim usuniesz kolumnę\n",
    "    original_prices = None\n",
    "    if 'Price' in data_for_prediction_final.columns:\n",
    "        original_prices = data_for_prediction_final['Price'].copy() # Zapisujemy oryginalne ceny\n",
    "        data_for_prediction_final = data_for_prediction_final.drop(columns=['Price']) # Usuwamy kolumnę Price\n",
    "        print(\"Usunięto kolumnę 'Price' z danych przekazywanych do predykcji.\")\n",
    "\n",
    "    original_predictions = predict_model(best_final_model, data=data_for_prediction_final)\n",
    "    \n",
    "    print(\"\\nPierwsze wiersze predykcji dla df_original_processed:\")\n",
    "    display(original_predictions.head())\n",
    "    \n",
    "    # Jeśli chcesz dodać z powrotem oryginalne ceny (dla porównania)\n",
    "    if original_prices is not None:\n",
    "        # Upewnij się, że indeksy pasują. original_predictions powinien mieć ten sam indeks co data_for_prediction_final\n",
    "        original_predictions['Original_Price'] = original_prices.loc[original_predictions.index] \n",
    "        print(\"\\nPierwsze wiersze predykcji wraz z oryginalną ceną (jeśli była dostępna):\")\n",
    "        display(original_predictions[['Original_Price', 'prediction_label']].head())\n",
    "else:\n",
    "    if not best_final_model:\n",
    "        print(\"Model (best_final_model) nie został poprawnie załadowany lub sfinalizowany.\")\n",
    "    if df_original_processed.empty:\n",
    "        print(\"DataFrame df_original_processed jest pusty, nie można wykonać predykcji.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b20ea6f-510c-4e2d-a0cf-bae03d3c5a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Komórka 43 (poprzednio 88) - ZAPIS WYNIKÓW (po Opcji 1 w predykcji)\n",
    "if original_predictions is not None:\n",
    "    # original_predictions ma indeksy z df_original_processed (po usunięciu NaN w Area/Location i outlierów w Area)\n",
    "    # oraz kolumnę 'prediction_label'. Może też mieć inne kolumny z df_original_processed (bez Price).\n",
    "    \n",
    "    # Chcemy dodać 'prediction_label' do oryginalnego df_original\n",
    "    \n",
    "    # 1. Stwórz DataFrame tylko z SaleId (jeśli jest w indeksie original_predictions, zresetuj go)\n",
    "    #    i prediction_label\n",
    "    if original_predictions.index.name == 'SaleId': # Jeśli SaleId jest indeksem\n",
    "        predictions_to_merge = original_predictions[['prediction_label']].reset_index()\n",
    "    elif 'SaleId' in original_predictions.columns: # Jeśli SaleId jest kolumną\n",
    "         predictions_to_merge = original_predictions[['SaleId', 'prediction_label']].copy()\n",
    "    else: # Jeśli SaleId nie ma, a indeks jest numeryczny, musimy go odzyskać\n",
    "        # To jest bardziej skomplikowane, jeśli SaleId nie było zachowane w df_original_processed\n",
    "        # Załóżmy, że SaleId było w df_original_processed i zostało przeniesione do original_predictions\n",
    "        # Jeśli nie, trzeba by wrócić do df_original_processed i wyciągnąć SaleId z niego po indeksie.\n",
    "        # Dla uproszczenia zakładam, że SaleId jest w original_predictions\n",
    "        print(\"OSTRZEŻENIE: Brak kolumny 'SaleId' w original_predictions. Łączenie może być niepoprawne.\")\n",
    "        # Spróbuj użyć indeksu, jeśli odpowiada SaleId z df_original_processed\n",
    "        predictions_to_merge = pd.DataFrame({\n",
    "            'SaleId': df_original_processed.loc[original_predictions.index, 'SaleId'], \n",
    "            'prediction_label': original_predictions['prediction_label']\n",
    "        })\n",
    "\n",
    "    predictions_to_merge.rename(columns={'prediction_label': 'PredictedPrice'}, inplace=True)\n",
    "    \n",
    "    # 2. Połącz z oryginalnym df_original\n",
    "    df_original_with_all_predictions = pd.merge(\n",
    "        df_original, # Używamy oryginalnego df_original\n",
    "        predictions_to_merge,\n",
    "        on='SaleId',\n",
    "        how='left' \n",
    "    )\n",
    "    \n",
    "    # 3. Przenieś kolumnę 'PredictedPrice_LGBM'\n",
    "    if 'Price' in df_original_with_all_predictions.columns and 'PredictedPrice' in df_original_with_all_predictions.columns:\n",
    "        cols = list(df_original_with_all_predictions.columns)\n",
    "        price_index = cols.index('Price')\n",
    "        # Upewnij się, że 'PredictedPrice_LGBM' nie jest już w liście, zanim ją wstawisz\n",
    "        if 'PredictedPrice' in cols:\n",
    "            cols.remove('PredictedPrice')\n",
    "        cols.insert(price_index + 1, 'PredictedPrice')\n",
    "        df_original_with_all_predictions = df_original_with_all_predictions[cols]\n",
    "        \n",
    "    print(\"\\nPierwsze wiersze df_original z dodanymi predykcjami:\")\n",
    "    display(df_original_with_all_predictions.head())\n",
    "    \n",
    "    # 4. Zapisz do CSV\n",
    "    df_original_with_all_predictions.to_csv('sale_2024_0_predict.csv', index=False)\n",
    "    print(\"\\nPlik 'sale_2024_0_with_100_predictions.csv' został zapisany.\")\n",
    "    \n",
    "    print(\"\\nPrzykładowe wiersze z oryginalną ceną i predykcją (gdzie predykcja istnieje):\")\n",
    "    display(df_original_with_all_predictions[df_original_with_all_predictions['PredictedPrice'].notna()][['SaleId', 'Price', 'PredictedPrice']].head(20))\n",
    "    \n",
    "else:\n",
    "    print(\"Brak predykcji do zapisania (original_predictions jest None).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ccb4d4-0a8f-4f7c-960c-f1a9657fa7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original_with_all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c4f0d6-be27-4c52-b794-2fdf676d8595",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
