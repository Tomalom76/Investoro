{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43863fe1",
   "metadata": {},
   "source": [
    "\n",
    "# Hedonic Housing Valuation: Hierarchical Routing (ppm) + Quantile GBM\n",
    "\n",
    "Notebook implements two variants requested: NN routing (classifier → per‑segment regressors on log price-per-m²) and production‑friendly GBM with quantile uncertainty. Configure in the next cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4e5768c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== Imports ====\n",
    "import os, re, json, math, gc, random, warnings\n",
    "from datetime import datetime\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "try:\n",
    "    import h3\n",
    "except Exception:\n",
    "    h3 = None\n",
    "\n",
    "SEED = SEED\n",
    "random.seed(SEED); np.random.seed(SEED); tf.keras.utils.set_random_seed(SEED)\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==== Utility helpers ====\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = (s or '').lower()\n",
    "    patterns = [\n",
    "        r'oferta nie stanowi.*?kodeksu cywilnego', r'prosz[ąa] o kontakt.*',\n",
    "        r'tylko u nas.*', r'nie pobieramy prowizji.*', r'bez prowizji.*', r'kupuj.*bezpieczni.*'\n",
    "    ]\n",
    "    for p in patterns:\n",
    "        s = re.sub(p, ' ', s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r'[^a-zA-Ząćęłńóśźż0-9\\s]', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "# Derive city slug from Predicted_Loc like \"Miasto -> Dzielnica -> ...\"\n",
    "\n",
    "def city_from_loc(loc: str) -> str:\n",
    "    if not isinstance(loc, str) or not loc:\n",
    "        return 'unknown'\n",
    "    city = loc.split('->')[0].strip()\n",
    "    city = city.lower().replace(' ', '_')\n",
    "    city = re.sub(r'[^a-z_ąćęłńóśźż]', '', city)\n",
    "    return city if city else 'unknown'\n",
    "\n",
    "# Safe numeric conversion\n",
    "\n",
    "def to_num(s):\n",
    "    try:\n",
    "        return float(str(s).replace(' ', '').replace(',', '.'))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# Simple metrics\n",
    "\n",
    "def median_ape(y_true, y_pred):\n",
    "    ape = np.abs((y_true - y_pred) / np.clip(y_true, 1e-9, None))\n",
    "    return float(np.median(ape))\n",
    "\n",
    "print('Imports ready.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a0aac1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready — update DATA_PATH as needed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== Config ====\n",
    "DATA_PATH = 'Data_state_LSTM_predicted_full_v4_FINAL.csv'  # CSV UTF-8-SIG with ';' separator\n",
    "SAVE_DIR = 'artifacts_ppm_routing'\n",
    "MODEL_VARIANT = 'BOTH'  # 'NN', 'GBM', or 'BOTH'\n",
    "\n",
    "# Binning / grouping\n",
    "N_BINS = 10\n",
    "MIN_SAMPLES_GROUP = 150\n",
    "GEO_MODE = 'city'  # 'city' or 'h3' (h3 requires lat/lon columns)\n",
    "H3_RESOLUTION = 8   # ~0.46 km cells (adjust 8–9 to target 0.3–0.7 km)\n",
    "\n",
    "# Validation & randomness\n",
    "SEED = 42\n",
    "TIME_COL = 'PublishDate'  # optional; if missing, stratified random split is used\n",
    "VAL_FRACTION_TIME = 0.2   # last 20% by time\n",
    "\n",
    "# Uncertainty band (alpha = relative half‑width around median on ppm scale)\n",
    "ALPHA_PCT = 0.10  # 10%\n",
    "MC_SAMPLES = 50   # for NN MC Dropout\n",
    "\n",
    "# Text settings\n",
    "TFIDF_MAX_FEATURES = 3000  # for GBM variant\n",
    "TEXT_MAX_TOKENS = 3000     # for NN TextVectorization (multi_hot)\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print('Config ready — update DATA_PATH as needed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "186d51cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ai\\AppData\\Local\\miniconda3\\envs\\projekt_stan\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ai\\AppData\\Local\\miniconda3\\envs\\projekt_stan\\Lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.19.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies checked.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== (Optional) Install dependencies if missing ====\n",
    "import importlib, subprocess, sys\n",
    "\n",
    "def ensure(pkg, pip_name=None):\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "    except Exception:\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', pip_name or pkg])\n",
    "\n",
    "# Core\n",
    "for pkg in ['numpy','pandas','scikit-learn','tensorflow']:\n",
    "    ensure(pkg)\n",
    "# Geo and GBM\n",
    "ensure('h3', 'h3')\n",
    "ensure('geohash2', 'geohash2')\n",
    "ensure('lightgbm', 'lightgbm')\n",
    "ensure('xgboost', 'xgboost')\n",
    "ensure('tensorflow_addons','tensorflow-addons')\n",
    "\n",
    "print('Dependencies checked.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ab3ad81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geo unit used for grouping: city_slug\n",
      "Prepared rows: 1223662\n"
     ]
    }
   ],
   "source": [
    "# ==== Load CSV (header-agnostic with positional fallback) v3 ====\n",
    "import pandas as pd, numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "expected_cols = ['SaleId','Title','Description','Area','Price','NumberOfRooms','BuiltYear','BuildingType',\n",
    "                 'OfferFrom','Floor','Floors','TypeOfMarket','Type','Predict_State','Predicted_Loc','pricepermeter']\n",
    "\n",
    "index_map = {\n",
    "    0:'SaleId', 3:'Title', 4:'Description', 5:'Area', 6:'Price',\n",
    "    11:'NumberOfRooms', 12:'BuiltYear', 14:'BuildingType', 16:'OfferFrom',\n",
    "    17:'Floor', 18:'Floors', 19:'TypeOfMarket', 28:'Type', 54:'Predicted_Loc', 55:'Predict_State'\n",
    "}\n",
    "\n",
    "# 1) Try regular read with header row\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH, sep=';', encoding='utf-8-sig', low_memory=False)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f'Failed to read CSV at {DATA_PATH}: {e}')\n",
    "\n",
    "# 2) If expected columns are missing, fallback to positional mapping without any heuristics\n",
    "if not set(expected_cols[:-1]).issubset(df.columns):\n",
    "    def map_by_index(df_raw):\n",
    "        present = [k for k in index_map.keys() if k < df_raw.shape[1]]\n",
    "        if not present:\n",
    "            raise ValueError('None of the expected indices are present in the raw frame.')\n",
    "        out = df_raw.iloc[:, present].copy()\n",
    "        out.columns = [index_map[k] for k in present]\n",
    "        return out\n",
    "\n",
    "    fallback_success = False\n",
    "    for skip in (1, 0):  # prefer skipping first row (likely header); then try without skipping\n",
    "        try:\n",
    "            df_raw = pd.read_csv(DATA_PATH, sep=';', encoding='utf-8-sig', header=None, low_memory=False, skiprows=skip)\n",
    "            df = map_by_index(df_raw)\n",
    "            fallback_success = True\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "    if not fallback_success:\n",
    "        raise RuntimeError(f'Fallback read failed: {last_err}')\n",
    "\n",
    "# 3) Ensure all expected columns exist with safe defaults\n",
    "n = len(df)\n",
    "for c in expected_cols:\n",
    "    if c not in df.columns:\n",
    "        if c in ['Title','Description','Predicted_Loc','BuildingType','OfferFrom','TypeOfMarket','Type','Predict_State']:\n",
    "            df[c] = pd.Series(['']*n, dtype='string')\n",
    "        elif c in ['Area','Price','NumberOfRooms','BuiltYear','Floor','Floors']:\n",
    "            df[c] = np.nan\n",
    "        elif c == 'pricepermeter':\n",
    "            df[c] = np.nan\n",
    "\n",
    "# 4) Numerics and text clean\n",
    "for c in ['Price','Area','NumberOfRooms','BuiltYear','Floor','Floors']:\n",
    "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "df['Title'] = df['Title'].astype('string').fillna('')\n",
    "df['Description'] = df['Description'].astype('string').fillna('').apply(clean_text)\n",
    "\n",
    "# 5) ppm\n",
    "if df['pricepermeter'].isna().all():\n",
    "    df['pricepermeter'] = np.where(df['Area']>0, df['Price']/df['Area'], np.nan)\n",
    "\n",
    "# 6) Basic filters\n",
    "df = df[(df['Price']>1000) & (df['Area']>0)].copy()\n",
    "\n",
    "# 7) Outlier trimming\n",
    "p1_P, p99_P = df['Price'].quantile([0.01,0.99])\n",
    "p1_ppm, p99_ppm = df['pricepermeter'].quantile([0.01,0.99])\n",
    "df = df[(df['Price'].between(p1_P, p99_P)) & (df['pricepermeter'].between(p1_ppm, p99_ppm))].copy()\n",
    "\n",
    "# 8) BuildingAge\n",
    "year_now = datetime.now().year\n",
    "by = df['BuiltYear'].copy()\n",
    "med_year = by.dropna().median() if not by.dropna().empty else 2000\n",
    "by = by.fillna(med_year).clip(1800, year_now+1)\n",
    "df['BuildingAge'] = np.maximum(0, year_now - by).astype(int)\n",
    "\n",
    "# 9) Categoricals\n",
    "for c in ['Predict_State','Predicted_Loc','BuildingType','TypeOfMarket','Type','OfferFrom']:\n",
    "    df[c] = df[c].astype('string').fillna('unknown').replace({'nan':'unknown','None':'unknown'})\n",
    "\n",
    "# 10) city_slug / h3\n",
    "df['city_slug'] = df['Predicted_Loc'].apply(city_from_loc)\n",
    "if GEO_MODE=='h3' and 'Latitude' in df.columns and 'Longitude' in df.columns:\n",
    "    import h3\n",
    "    df['h3_cell'] = df.apply(lambda r: h3.geo_to_h3(r['Latitude'], r['Longitude'], H3_RESOLUTION), axis=1)\n",
    "else:\n",
    "    df['h3_cell'] = 'na'\n",
    "\n",
    "# 11) Geo unit and target\n",
    "geo_unit = 'h3_cell' if GEO_MODE=='h3' and df['h3_cell'].nunique()>1 else 'city_slug'\n",
    "print('Geo unit used for grouping:', geo_unit)\n",
    "\n",
    "df['Price_log_m2'] = np.log1p(df['pricepermeter'])\n",
    "print('Prepared rows:', len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a6857b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique groups: 7430\n",
      "GroupLabel\n",
      "warszawa__bin2    14234\n",
      "warszawa__bin5    14101\n",
      "warszawa__bin0    14099\n",
      "warszawa__bin7    14099\n",
      "warszawa__bin4    14098\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== Binning per geography ====\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Ensure minimum samples and quantile bins per geo\n",
    "bins = []\n",
    "for g, gdf in df.groupby(geo_unit):\n",
    "    if len(gdf) < max(MIN_SAMPLES_GROUP, N_BINS*10):\n",
    "        continue\n",
    "    q = np.linspace(0,1,N_BINS+1)\n",
    "    edges = np.unique(np.quantile(gdf['pricepermeter'], q))\n",
    "    if len(edges) < 3:\n",
    "        continue\n",
    "    b = np.digitize(gdf['pricepermeter'], edges[1:-1], right=True)\n",
    "    label = [f\"{g}__bin{bi}\" for bi in b]\n",
    "    bins.append(pd.DataFrame({'idx': gdf.index, 'GroupLabel': label}))\n",
    "\n",
    "if bins:\n",
    "    bin_df = pd.concat(bins, ignore_index=True)\n",
    "    df = df.join(bin_df.set_index('idx'), how='left')\n",
    "\n",
    "mask_missing = df['GroupLabel'].isna()\n",
    "if mask_missing.any():\n",
    "    for g, gdf in df[mask_missing].groupby('city_slug'):\n",
    "        if len(gdf) < N_BINS*5:\n",
    "            df.loc[gdf.index, 'GroupLabel'] = f\"{g}__bin0\"\n",
    "            continue\n",
    "        edges = np.unique(np.quantile(gdf['pricepermeter'], np.linspace(0,1,N_BINS+1)))\n",
    "        b = np.digitize(gdf['pricepermeter'], edges[1:-1], right=True)\n",
    "        df.loc[gdf.index, 'GroupLabel'] = [f\"{g}__bin{bi}\" for bi in b]\n",
    "\n",
    "counts = df['GroupLabel'].value_counts()\n",
    "rare = set(counts[counts < MIN_SAMPLES_GROUP].index)\n",
    "if rare:\n",
    "    df['GroupLabel'] = df['GroupLabel'].apply(lambda x: x if x not in rare else f\"{str(x).split('__')[0]}__bin0\")\n",
    "\n",
    "print('Unique groups:', df['GroupLabel'].nunique())\n",
    "print(df['GroupLabel'].value_counts().head())\n",
    "\n",
    "groups = sorted(df['GroupLabel'].unique())\n",
    "group_to_id = {g:i for i,g in enumerate(groups)}\n",
    "id_to_group = {i:g for g,i in group_to_id.items()}\n",
    "with open(os.path.join(SAVE_DIR,'group_labels.json'),'w',encoding='utf-8') as f:\n",
    "    json.dump({'group_to_id':group_to_id,'id_to_group':id_to_group}, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e78821c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Rebuild label mapping after any relabeling and guard against NaNs ====\n",
    "# Recompute mapping on the full df so that 'other' and any fallback labels are included\n",
    "groups = sorted(df['GroupLabel'].unique())\n",
    "group_to_id = {g: i for i, g in enumerate(groups)}\n",
    "id_to_group = {i: g for g, i in group_to_id.items()}\n",
    "\n",
    "# (Optional) save mapping\n",
    "import os, json\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "with open(os.path.join(SAVE_DIR,'group_labels.json'),'w',encoding='utf-8') as f:\n",
    "    json.dump({'group_to_id':group_to_id,'id_to_group':id_to_group}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ==== Safe dataset creators that drop rows with unmapped labels (should be none, but safe) ====\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Ensure TEXT_MAX_TOKENS exists\n",
    "try:\n",
    "    TEXT_MAX_TOKENS\n",
    "except NameError:\n",
    "    TEXT_MAX_TOKENS = 3000\n",
    "\n",
    "# Build an adapt dataset if needed later (text/vectorizers)\n",
    "adapt_dict = tf.data.Dataset.from_tensor_slices(dict(train_df[NUMERIC + CATEG + ['Title','Description']])).batch(256)\n",
    "\n",
    "def ds_from(df_):\n",
    "    y_map = df_['GroupLabel'].map(group_to_id)\n",
    "    ok = y_map.notna()\n",
    "    dfx = df_.loc[ok]\n",
    "    y = y_map.loc[ok].astype('int32').values\n",
    "    xdict = {f: dfx[f].values for f in NUMERIC+CATEG}\n",
    "    xdict['text_all'] = (dfx['Title'].fillna('') + ' ' + dfx['Description'].fillna('')).values\n",
    "    ds = tf.data.Dataset.from_tensor_slices((xdict, y)).shuffle(len(dfx), seed=SEED).batch(256).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = ds_from(train_df)\n",
    "val_ds = ds_from(val_df)\n",
    "\n",
    "# When computing class weights, also guard against NaNs\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "y_train_all = train_df['GroupLabel'].map(group_to_id)\n",
    "y_train = y_train_all[y_train_all.notna()].astype('int32').values\n",
    "classes = np.arange(len(group_to_id))\n",
    "class_weight = {int(i): float(w) for i,w in enumerate(compute_class_weight(class_weight='balanced', classes=classes, y=y_train))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d88867f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared feature columns.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== Feature schema ====\n",
    "NUMERIC = ['Area','NumberOfRooms','Floor','Floors','BuildingAge']\n",
    "CATEG = ['Predict_State','Predicted_Loc','BuildingType','TypeOfMarket','Type','OfferFrom','city_slug']\n",
    "TEXT_COLS = ['Title','Description']\n",
    "\n",
    "CLASSIFIER_INPUTS = NUMERIC + CATEG + TEXT_COLS\n",
    "REG_INPUTS = CLASSIFIER_INPUTS.copy()\n",
    "\n",
    "for c in NUMERIC:\n",
    "    if c not in train_df.columns:\n",
    "        train_df[c] = train_df[c].median(); val_df[c] = val_df[c].median()\n",
    "\n",
    "for c in CATEG:\n",
    "    for d in (train_df,val_df):\n",
    "        if c not in d.columns:\n",
    "            d[c] = 'unknown'\n",
    "        d[c] = d[c].astype(str).fillna('unknown')\n",
    "\n",
    "for c in TEXT_COLS:\n",
    "    for d in (train_df,val_df):\n",
    "        if c not in d.columns:\n",
    "            d[c] = ''\n",
    "        d[c] = d[c].astype(str)\n",
    "\n",
    "print('Prepared feature columns.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "698e7f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m3824/3824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4242s\u001b[0m 1s/step - accuracy: 7.6985e-04 - loss: 8.6267 - val_accuracy: 4.0861e-06 - val_loss: 8.5075 - learning_rate: 0.0010\n",
      "Epoch 2/25\n",
      "\u001b[1m3824/3824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4314s\u001b[0m 1s/step - accuracy: 0.0000e+00 - loss: 8.6835 - val_accuracy: 4.0861e-06 - val_loss: 8.5521 - learning_rate: 0.0010\n",
      "Epoch 3/25\n",
      "\u001b[1m3824/3824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4385s\u001b[0m 1s/step - accuracy: 2.8340e-06 - loss: 8.6737 - val_accuracy: 4.0861e-06 - val_loss: 8.5641 - learning_rate: 0.0010\n",
      "Epoch 4/25\n",
      "\u001b[1m3824/3824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4295s\u001b[0m 1s/step - accuracy: 7.3671e-07 - loss: 8.6037 - val_accuracy: 4.0861e-06 - val_loss: 8.5719 - learning_rate: 0.0010\n",
      "Epoch 5/25\n",
      "\u001b[1m3824/3824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4243s\u001b[0m 1s/step - accuracy: 7.6090e-06 - loss: 8.6592 - val_accuracy: 4.0861e-06 - val_loss: 8.5729 - learning_rate: 2.0000e-04\n",
      "Epoch 6/25\n",
      "\u001b[1m3824/3824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4284s\u001b[0m 1s/step - accuracy: 3.3124e-06 - loss: 8.5872 - val_accuracy: 4.0861e-06 - val_loss: 8.5741 - learning_rate: 2.0000e-04\n",
      "UnicodeEncodeError during .keras save; falling back to SavedModel: 'charmap' codec can't encode character '\\xb2' in position 1612: character maps to <undefined>\n",
      "INFO:tensorflow:Assets written to: artifacts_ppm_routing\\routing_classifier_sm\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: artifacts_ppm_routing\\routing_classifier_sm\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'artifacts_ppm_routing\\routing_classifier_sm'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): Dict[['Area', TensorSpec(shape=(None, 1), dtype=tf.float32, name='Area')], ['NumberOfRooms', TensorSpec(shape=(None, 1), dtype=tf.float32, name='NumberOfRooms')], ['Floor', TensorSpec(shape=(None, 1), dtype=tf.float32, name='Floor')], ['Floors', TensorSpec(shape=(None, 1), dtype=tf.float32, name='Floors')], ['BuildingAge', TensorSpec(shape=(None, 1), dtype=tf.float32, name='BuildingAge')], ['Predict_State', TensorSpec(shape=(None, 1), dtype=tf.string, name='Predict_State')], ['Predicted_Loc', TensorSpec(shape=(None, 1), dtype=tf.string, name='Predicted_Loc')], ['BuildingType', TensorSpec(shape=(None, 1), dtype=tf.string, name='BuildingType')], ['TypeOfMarket', TensorSpec(shape=(None, 1), dtype=tf.string, name='TypeOfMarket')], ['Type', TensorSpec(shape=(None, 1), dtype=tf.string, name='Type')], ['OfferFrom', TensorSpec(shape=(None, 1), dtype=tf.string, name='OfferFrom')], ['city_slug', TensorSpec(shape=(None, 1), dtype=tf.string, name='city_slug')], ['text_all', TensorSpec(shape=(None, 1), dtype=tf.string, name='text_all')]]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 5499), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1900766371024: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1900766371792: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1900766370832: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1900766372944: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1900766371984: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1900766373904: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1900766370256: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1900766375056: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1892705484432: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1900766374672: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1892705483856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1892705483664: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1892705482512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1892705483472: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1892705481168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1892705480784: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1892705479632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1892705480400: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1892705477712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1892705478864: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1892705476176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1892705476944: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1892705474640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1892705475408: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1900220957328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1900220958480: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1900766365072: TensorSpec(shape=(), dtype=tf.string, name=None)\n",
      "  1900766374096: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1900766375824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1900220956752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1900220958672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1900220959248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1900220956944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1900220958864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Saved SavedModel at: artifacts_ppm_routing\\routing_classifier_sm\n",
      "{'accuracy': 4.086085652527448e-06, 'macro_f1': 1.8160306473332044e-09, 'balanced_acc': 0.00022222222222222223}\n"
     ]
    }
   ],
   "source": [
    "# ==== NN Classifier for GroupLabel (with robust Windows-safe saving) ====\n",
    "if MODEL_VARIANT in ('NN','BOTH'):\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score, confusion_matrix\n",
    "\n",
    "    # Inputs and preprocessing layers\n",
    "    inputs = {}\n",
    "    encoded = []\n",
    "\n",
    "    # Dataset for adapting preprocessing layers\n",
    "    adapt_dict = tf.data.Dataset.from_tensor_slices(dict(train_df[NUMERIC + CATEG + ['Title','Description']])).batch(256)\n",
    "\n",
    "    # Numeric\n",
    "    for f in NUMERIC:\n",
    "        inp = keras.Input(shape=(1,), name=f, dtype=tf.float32); inputs[f] = inp\n",
    "        layer_norm = layers.Normalization(axis=-1)\n",
    "        layer_norm.adapt(adapt_dict.map(lambda x: tf.expand_dims(tf.cast(x[f], tf.float32), -1)))\n",
    "        encoded.append(layer_norm(inp))\n",
    "\n",
    "    # Categorical (one-hot via StringLookup)\n",
    "    for f in CATEG:\n",
    "        inp = keras.Input(shape=(1,), name=f, dtype=tf.string); inputs[f] = inp\n",
    "        sl = layers.StringLookup(output_mode='one_hot')\n",
    "        sl.adapt(adapt_dict.map(lambda x: x[f]))\n",
    "        encoded.append(sl(inp))\n",
    "\n",
    "    # Text feature: Title + Description\n",
    "    text_input = keras.Input(shape=(1,), name='text_all', dtype=tf.string)\n",
    "    tv = layers.TextVectorization(max_tokens=TEXT_MAX_TOKENS, output_mode='multi_hot')\n",
    "    tv.adapt(tf.data.Dataset.from_tensor_slices((train_df['Title'].fillna('') + ' ' + train_df['Description'].fillna(''))).batch(256))\n",
    "    encoded.append(tv(text_input))\n",
    "    inputs['text_all'] = text_input\n",
    "\n",
    "    # Classifier body\n",
    "    x = layers.Concatenate()(encoded)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(len(group_to_id), activation='softmax', name='class')(x)\n",
    "\n",
    "    clf = keras.Model(inputs, out)\n",
    "    clf.compile(optimizer=keras.optimizers.Adam(1e-3), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Input pipeline\n",
    "    def ds_from(df_):\n",
    "        y_map = df_['GroupLabel'].map(group_to_id)\n",
    "        ok = y_map.notna()\n",
    "        dfx = df_.loc[ok]\n",
    "        y = y_map.loc[ok].astype('int32').values\n",
    "        xdict = {f: dfx[f].values for f in NUMERIC+CATEG}\n",
    "        xdict['text_all'] = (dfx['Title'].fillna('') + ' ' + dfx['Description'].fillna('')).values\n",
    "        return tf.data.Dataset.from_tensor_slices((xdict, y)).shuffle(len(dfx), seed=SEED).batch(256).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    train_ds = ds_from(train_df)\n",
    "    val_ds = ds_from(val_df)\n",
    "\n",
    "    # Class weights (balanced)\n",
    "    y_train_all = train_df['GroupLabel'].map(group_to_id)\n",
    "    y_train = y_train_all[y_train_all.notna()].astype('int32').values\n",
    "    classes = np.arange(len(group_to_id))\n",
    "    class_weight = {int(i): float(w) for i,w in enumerate(compute_class_weight(class_weight='balanced', classes=classes, y=y_train))}\n",
    "\n",
    "    # Callbacks\n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    rlr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
    "    csvlog = keras.callbacks.CSVLogger(os.path.join(SAVE_DIR,'clf_training_log.csv'))\n",
    "\n",
    "    # Train\n",
    "    history = clf.fit(train_ds, validation_data=val_ds, epochs=25, callbacks=[es, rlr, csvlog], class_weight=class_weight)\n",
    "\n",
    "    # Robust save on Windows (CP1250): try .keras, fall back to SavedModel\n",
    "    save_base = os.path.join(SAVE_DIR, 'routing_classifier')\n",
    "    try:\n",
    "        clf.save(save_base + '.keras')\n",
    "        print('Saved .keras at:', save_base + '.keras')\n",
    "    except UnicodeEncodeError as e:\n",
    "        print('UnicodeEncodeError during .keras save; falling back to SavedModel:', e)\n",
    "        try:\n",
    "            # Keras 3 preferred API\n",
    "            clf.export(save_base + '_sm')\n",
    "            print('Saved SavedModel at:', save_base + '_sm')\n",
    "        except Exception:\n",
    "            # Legacy TF fallback\n",
    "            tf.saved_model.save(clf, save_base + '_sm')\n",
    "            print('Saved (legacy) TF SavedModel at:', save_base + '_sm')\n",
    "\n",
    "    # Simple eval on val for sanity\n",
    "    def predict_proba(df_):\n",
    "        xdict = {f: df_[f].values for f in NUMERIC+CATEG}\n",
    "        xdict['text_all'] = (df_['Title'].fillna('')+' '+df_['Description'].fillna('')).values\n",
    "        proba = clf.predict(xdict, batch_size=512, verbose=0)\n",
    "        return proba\n",
    "\n",
    "    proba_val = predict_proba(val_df)\n",
    "    y_true = val_df['GroupLabel'].map(group_to_id).values\n",
    "    y_pred = proba_val.argmax(axis=1)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1m = f1_score(y_true, y_pred, average='macro')\n",
    "    bacc = balanced_accuracy_score(y_true, y_pred)\n",
    "    print({'accuracy':acc, 'macro_f1':f1m, 'balanced_acc':bacc})\n",
    "else:\n",
    "    print('NN routing skipped by config.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b7e4cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training global fallback regressor...\n",
      "Epoch 1/30\n",
      "\u001b[1m3824/3824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1823s\u001b[0m 456ms/step - loss: 69.8288 - rmse: 8.3413 - val_loss: 32.5372 - val_rmse: 5.7041\n",
      "Epoch 2/30\n",
      "\u001b[1m3824/3824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1811s\u001b[0m 454ms/step - loss: 24.4612 - rmse: 4.9269 - val_loss: 6.7689 - val_rmse: 2.6017\n",
      "Epoch 3/30\n",
      "\u001b[1m3824/3824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1796s\u001b[0m 450ms/step - loss: 4.1205 - rmse: 2.0060 - val_loss: 0.3021 - val_rmse: 0.5497\n",
      "Epoch 4/30\n",
      "\u001b[1m3824/3824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1811s\u001b[0m 454ms/step - loss: 0.2277 - rmse: 0.4766 - val_loss: 0.1948 - val_rmse: 0.4414\n",
      "Epoch 5/30\n",
      "\u001b[1m3824/3824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1826s\u001b[0m 458ms/step - loss: 0.1944 - rmse: 0.4410 - val_loss: 0.1948 - val_rmse: 0.4414\n",
      "Epoch 6/30\n",
      "\u001b[1m3824/3824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1859s\u001b[0m 467ms/step - loss: 0.1947 - rmse: 0.4412 - val_loss: 0.1948 - val_rmse: 0.4414\n",
      "Epoch 7/30\n",
      "\u001b[1m3824/3824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1817s\u001b[0m 455ms/step - loss: 0.1949 - rmse: 0.4415 - val_loss: 0.1948 - val_rmse: 0.4414\n",
      "Epoch 8/30\n",
      "\u001b[1m3824/3824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1810s\u001b[0m 454ms/step - loss: 0.1949 - rmse: 0.4414 - val_loss: 0.1948 - val_rmse: 0.4414\n",
      "Epoch 9/30\n",
      "\u001b[1m3824/3824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1822s\u001b[0m 457ms/step - loss: 0.1947 - rmse: 0.4412 - val_loss: 0.1948 - val_rmse: 0.4414\n",
      "Epoch 10/30\n",
      "\u001b[1m3824/3824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1861s\u001b[0m 468ms/step - loss: 0.1955 - rmse: 0.4421 - val_loss: 0.1948 - val_rmse: 0.4414\n",
      "Epoch 11/30\n",
      "\u001b[1m3824/3824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1854s\u001b[0m 465ms/step - loss: 0.1944 - rmse: 0.4409 - val_loss: 0.1948 - val_rmse: 0.4414\n",
      "Epoch 12/30\n",
      "\u001b[1m3824/3824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1849s\u001b[0m 463ms/step - loss: 0.1949 - rmse: 0.4415 - val_loss: 0.1948 - val_rmse: 0.4414\n",
      "UnicodeEncodeError during .keras save; falling back to SavedModel: 'charmap' codec can't encode character '\\xb2' in position 1612: character maps to <undefined>\n",
      "INFO:tensorflow:Assets written to: artifacts_ppm_routing\\regressor_global_sm\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: artifacts_ppm_routing\\regressor_global_sm\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'artifacts_ppm_routing\\regressor_global_sm'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): Dict[['Area', TensorSpec(shape=(None, 1), dtype=tf.float32, name='Area')], ['NumberOfRooms', TensorSpec(shape=(None, 1), dtype=tf.float32, name='NumberOfRooms')], ['Floor', TensorSpec(shape=(None, 1), dtype=tf.float32, name='Floor')], ['Floors', TensorSpec(shape=(None, 1), dtype=tf.float32, name='Floors')], ['BuildingAge', TensorSpec(shape=(None, 1), dtype=tf.float32, name='BuildingAge')], ['Predict_State', TensorSpec(shape=(None, 1), dtype=tf.string, name='Predict_State')], ['Predicted_Loc', TensorSpec(shape=(None, 1), dtype=tf.string, name='Predicted_Loc')], ['BuildingType', TensorSpec(shape=(None, 1), dtype=tf.string, name='BuildingType')], ['TypeOfMarket', TensorSpec(shape=(None, 1), dtype=tf.string, name='TypeOfMarket')], ['Type', TensorSpec(shape=(None, 1), dtype=tf.string, name='Type')], ['OfferFrom', TensorSpec(shape=(None, 1), dtype=tf.string, name='OfferFrom')], ['city_slug', TensorSpec(shape=(None, 1), dtype=tf.string, name='city_slug')], ['text_all', TensorSpec(shape=(None, 1), dtype=tf.string, name='text_all')]]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1896598176208: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1896598178512: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1896598176016: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1896598179664: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1896598178704: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1896598180624: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1896598177744: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1896598181968: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1896598180240: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1896598183120: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1896598182160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896598180432: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1896598184848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896598184080: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1896598186576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896598185808: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1896598188112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896598187344: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1896598189264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896598179856: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1896598190800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896598190032: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1899815125648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1899815125264: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1892897537296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1892897536144: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1885984685520: TensorSpec(shape=(), dtype=tf.string, name=None)\n",
      "  1896598191952: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1896598191184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896598177552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1892897535952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1892897527504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1892897535760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1892897535568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1892897537488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1892897537104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Saved SavedModel at: artifacts_ppm_routing\\regressor_global_sm\n",
      "Training regressor for group aleksandrów_kujawski__bin0 (n=176)\n",
      "UnicodeEncodeError during .keras save; falling back to SavedModel: 'charmap' codec can't encode character '\\xb2' in position 5672: character maps to <undefined>\n",
      "INFO:tensorflow:Assets written to: artifacts_ppm_routing\\regressor__5946482021831562226_sm\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: artifacts_ppm_routing\\regressor__5946482021831562226_sm\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'artifacts_ppm_routing\\regressor__5946482021831562226_sm'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): Dict[['Area', TensorSpec(shape=(None, 1), dtype=tf.float32, name='Area')], ['NumberOfRooms', TensorSpec(shape=(None, 1), dtype=tf.float32, name='NumberOfRooms')], ['Floor', TensorSpec(shape=(None, 1), dtype=tf.float32, name='Floor')], ['Floors', TensorSpec(shape=(None, 1), dtype=tf.float32, name='Floors')], ['BuildingAge', TensorSpec(shape=(None, 1), dtype=tf.float32, name='BuildingAge')], ['Predict_State', TensorSpec(shape=(None, 1), dtype=tf.string, name='Predict_State')], ['Predicted_Loc', TensorSpec(shape=(None, 1), dtype=tf.string, name='Predicted_Loc')], ['BuildingType', TensorSpec(shape=(None, 1), dtype=tf.string, name='BuildingType')], ['TypeOfMarket', TensorSpec(shape=(None, 1), dtype=tf.string, name='TypeOfMarket')], ['Type', TensorSpec(shape=(None, 1), dtype=tf.string, name='Type')], ['OfferFrom', TensorSpec(shape=(None, 1), dtype=tf.string, name='OfferFrom')], ['city_slug', TensorSpec(shape=(None, 1), dtype=tf.string, name='city_slug')], ['text_all', TensorSpec(shape=(None, 1), dtype=tf.string, name='text_all')]]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1885643530384: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1885643537680: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1885643531920: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1885643529424: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1885643532880: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1885643532112: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1885643534032: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1885643532304: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1885643526544: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1885643534224: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1885643526160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1885643524048: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1896314510160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1885643523088: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1896314504592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896314504400: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1896314506128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896314505936: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1896314507856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896314507664: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1896314509392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896314509200: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1896314511312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896314511120: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1896314513232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896314514768: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1885643525392: TensorSpec(shape=(), dtype=tf.string, name=None)\n",
      "  1885643525008: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1885643537104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896314512848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896314509776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896314513040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896314514960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896314514000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896314512656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896314514384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Saved SavedModel at: artifacts_ppm_routing\\regressor__5946482021831562226_sm\n",
      "Training regressor for group aleksandrów_łódzki__bin0 (n=291)\n",
      "UnicodeEncodeError during .keras save; falling back to SavedModel: 'charmap' codec can't encode character '\\xb2' in position 1777: character maps to <undefined>\n",
      "INFO:tensorflow:Assets written to: artifacts_ppm_routing\\regressor__2439099339002759264_sm\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: artifacts_ppm_routing\\regressor__2439099339002759264_sm\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'artifacts_ppm_routing\\regressor__2439099339002759264_sm'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): Dict[['Area', TensorSpec(shape=(None, 1), dtype=tf.float32, name='Area')], ['NumberOfRooms', TensorSpec(shape=(None, 1), dtype=tf.float32, name='NumberOfRooms')], ['Floor', TensorSpec(shape=(None, 1), dtype=tf.float32, name='Floor')], ['Floors', TensorSpec(shape=(None, 1), dtype=tf.float32, name='Floors')], ['BuildingAge', TensorSpec(shape=(None, 1), dtype=tf.float32, name='BuildingAge')], ['Predict_State', TensorSpec(shape=(None, 1), dtype=tf.string, name='Predict_State')], ['Predicted_Loc', TensorSpec(shape=(None, 1), dtype=tf.string, name='Predicted_Loc')], ['BuildingType', TensorSpec(shape=(None, 1), dtype=tf.string, name='BuildingType')], ['TypeOfMarket', TensorSpec(shape=(None, 1), dtype=tf.string, name='TypeOfMarket')], ['Type', TensorSpec(shape=(None, 1), dtype=tf.string, name='Type')], ['OfferFrom', TensorSpec(shape=(None, 1), dtype=tf.string, name='OfferFrom')], ['city_slug', TensorSpec(shape=(None, 1), dtype=tf.string, name='city_slug')], ['text_all', TensorSpec(shape=(None, 1), dtype=tf.string, name='text_all')]]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1896314515536: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1896314518992: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1896314512464: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1896314511888: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1888360638864: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1896314518800: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1888360640208: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1888360635024: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1888360635792: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1888360628496: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1888360628304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1888360636560: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1888360629456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1888360629264: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1888360631184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1888360630992: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1888360644240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1888360644432: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1888360642704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1888360643088: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1888360641552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1888360641744: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1898989797648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1888360630224: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1898989798608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1898989813584: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1888360634448: TensorSpec(shape=(), dtype=tf.string, name=None)\n",
      "  1888360636176: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1896314516112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1896314516304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1888360638480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1898989798416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1898989799376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1898989813200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1898989799184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1898989813392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Saved SavedModel at: artifacts_ppm_routing\\regressor__2439099339002759264_sm\n",
      "Training regressor for group andrychów__bin0 (n=278)\n",
      "UnicodeEncodeError during .keras save; falling back to SavedModel: 'charmap' codec can't encode character '\\xb2' in position 5316: character maps to <undefined>\n",
      "INFO:tensorflow:Assets written to: artifacts_ppm_routing\\regressor__4717277491672629630_sm\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: artifacts_ppm_routing\\regressor__4717277491672629630_sm\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'artifacts_ppm_routing\\regressor__4717277491672629630_sm'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): Dict[['Area', TensorSpec(shape=(None, 1), dtype=tf.float32, name='Area')], ['NumberOfRooms', TensorSpec(shape=(None, 1), dtype=tf.float32, name='NumberOfRooms')], ['Floor', TensorSpec(shape=(None, 1), dtype=tf.float32, name='Floor')], ['Floors', TensorSpec(shape=(None, 1), dtype=tf.float32, name='Floors')], ['BuildingAge', TensorSpec(shape=(None, 1), dtype=tf.float32, name='BuildingAge')], ['Predict_State', TensorSpec(shape=(None, 1), dtype=tf.string, name='Predict_State')], ['Predicted_Loc', TensorSpec(shape=(None, 1), dtype=tf.string, name='Predicted_Loc')], ['BuildingType', TensorSpec(shape=(None, 1), dtype=tf.string, name='BuildingType')], ['TypeOfMarket', TensorSpec(shape=(None, 1), dtype=tf.string, name='TypeOfMarket')], ['Type', TensorSpec(shape=(None, 1), dtype=tf.string, name='Type')], ['OfferFrom', TensorSpec(shape=(None, 1), dtype=tf.string, name='OfferFrom')], ['city_slug', TensorSpec(shape=(None, 1), dtype=tf.string, name='city_slug')], ['text_all', TensorSpec(shape=(None, 1), dtype=tf.string, name='text_all')]]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1898989798224: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1898989801680: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1898989803024: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1898989806864: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1898989803600: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1898989799760: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1898989800144: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1898989811664: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1898989808784: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1898989809168: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1898989809552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1898989810128: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1898976783312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1898976783120: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1898976785616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1898976785040: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1898976786960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1898976786768: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1893599495184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1893599494800: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1893599496336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1893599496144: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1887800801808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1887800801616: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1892705340880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1892705343376: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1893599495376: TensorSpec(shape=(), dtype=tf.string, name=None)\n",
      "  1898976787920: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1898976788304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1898976787344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1887800802384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1898989802448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1898989802064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1898989807632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1892705340496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1892705342800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Saved SavedModel at: artifacts_ppm_routing\\regressor__4717277491672629630_sm\n",
      "WARNING:tensorflow:5 out of the last 482 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001BDA12F13A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 482 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001BDA12F13A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 484 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001BDA12F13A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 484 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001BDA12F13A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training regressor for group augustów__bin0 (n=479)\n",
      "UnicodeEncodeError during .keras save; falling back to SavedModel: 'charmap' codec can't encode character '\\xb2' in position 7273: character maps to <undefined>\n",
      "INFO:tensorflow:Assets written to: artifacts_ppm_routing\\regressor__8079749075666341245_sm\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: artifacts_ppm_routing\\regressor__8079749075666341245_sm\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'artifacts_ppm_routing\\regressor__8079749075666341245_sm'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): Dict[['Area', TensorSpec(shape=(None, 1), dtype=tf.float32, name='Area')], ['NumberOfRooms', TensorSpec(shape=(None, 1), dtype=tf.float32, name='NumberOfRooms')], ['Floor', TensorSpec(shape=(None, 1), dtype=tf.float32, name='Floor')], ['Floors', TensorSpec(shape=(None, 1), dtype=tf.float32, name='Floors')], ['BuildingAge', TensorSpec(shape=(None, 1), dtype=tf.float32, name='BuildingAge')], ['Predict_State', TensorSpec(shape=(None, 1), dtype=tf.string, name='Predict_State')], ['Predicted_Loc', TensorSpec(shape=(None, 1), dtype=tf.string, name='Predicted_Loc')], ['BuildingType', TensorSpec(shape=(None, 1), dtype=tf.string, name='BuildingType')], ['TypeOfMarket', TensorSpec(shape=(None, 1), dtype=tf.string, name='TypeOfMarket')], ['Type', TensorSpec(shape=(None, 1), dtype=tf.string, name='Type')], ['OfferFrom', TensorSpec(shape=(None, 1), dtype=tf.string, name='OfferFrom')], ['city_slug', TensorSpec(shape=(None, 1), dtype=tf.string, name='city_slug')], ['text_all', TensorSpec(shape=(None, 1), dtype=tf.string, name='text_all')]]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1896314518416: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1896314505168: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1896314517456: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1896314503632: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1896314518224: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1896314508048: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1888360641360: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1896314508624: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1888360638672: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1888360631952: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1888360630416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1888360632912: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1888360636368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1888360631568: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1888360631760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1888360630608: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1888360633104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1888360633296: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1888360635216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1888360639824: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1885643524240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1885643534992: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1885643537296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1885643536720: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1885643531344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1885643529232: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1888360640400: TensorSpec(shape=(), dtype=tf.string, name=None)\n",
      "  1896314515152: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1896314503248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1885643530768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1885643535952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1885643527504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1885643530192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1885643536912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1885643527120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1885643528656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Saved SavedModel at: artifacts_ppm_routing\\regressor__8079749075666341245_sm\n",
      "Training regressor for group banino__bin0 (n=166)\n",
      "UnicodeEncodeError during .keras save; falling back to SavedModel: 'charmap' codec can't encode character '\\xb2' in position 4734: character maps to <undefined>\n",
      "INFO:tensorflow:Assets written to: artifacts_ppm_routing\\regressor__2366006624724727952_sm\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: artifacts_ppm_routing\\regressor__2366006624724727952_sm\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'artifacts_ppm_routing\\regressor__2366006624724727952_sm'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): Dict[['Area', TensorSpec(shape=(None, 1), dtype=tf.float32, name='Area')], ['NumberOfRooms', TensorSpec(shape=(None, 1), dtype=tf.float32, name='NumberOfRooms')], ['Floor', TensorSpec(shape=(None, 1), dtype=tf.float32, name='Floor')], ['Floors', TensorSpec(shape=(None, 1), dtype=tf.float32, name='Floors')], ['BuildingAge', TensorSpec(shape=(None, 1), dtype=tf.float32, name='BuildingAge')], ['Predict_State', TensorSpec(shape=(None, 1), dtype=tf.string, name='Predict_State')], ['Predicted_Loc', TensorSpec(shape=(None, 1), dtype=tf.string, name='Predicted_Loc')], ['BuildingType', TensorSpec(shape=(None, 1), dtype=tf.string, name='BuildingType')], ['TypeOfMarket', TensorSpec(shape=(None, 1), dtype=tf.string, name='TypeOfMarket')], ['Type', TensorSpec(shape=(None, 1), dtype=tf.string, name='Type')], ['OfferFrom', TensorSpec(shape=(None, 1), dtype=tf.string, name='OfferFrom')], ['city_slug', TensorSpec(shape=(None, 1), dtype=tf.string, name='city_slug')], ['text_all', TensorSpec(shape=(None, 1), dtype=tf.string, name='text_all')]]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1885643530576: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1896314510928: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1885643534608: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1885643533648: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1885643533264: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1885643533072: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1892705337616: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1892705348368: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1892705347792: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1892705341648: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1892705338192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1892705340112: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1892705338000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1892705347408: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1892705351248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1892705351056: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1892705352976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1892705352784: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1884833317136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1892705342608: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1884833318864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1884833318672: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1884833320400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1884833320208: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1884833322320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1884833323856: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1892705353360: TensorSpec(shape=(), dtype=tf.string, name=None)\n",
      "  1892705352592: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1892705353552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1884833320976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1884833319248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1884833322128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1884833324048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1884833323088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1884833321360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1884833323472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Saved SavedModel at: artifacts_ppm_routing\\regressor__2366006624724727952_sm\n",
      "Training regressor for group barczewo__bin0 (n=162)\n",
      "UnicodeEncodeError during .keras save; falling back to SavedModel: 'charmap' codec can't encode character '\\xb2' in position 2299: character maps to <undefined>\n",
      "INFO:tensorflow:Assets written to: artifacts_ppm_routing\\regressor__5452600282512542450_sm\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: artifacts_ppm_routing\\regressor__5452600282512542450_sm\\assets\n"
     ]
    },
    
    
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: artifacts_ppm_routing\\regressor__4480766881104448127_sm\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'artifacts_ppm_routing\\regressor__4480766881104448127_sm'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): Dict[['Area', TensorSpec(shape=(None, 1), dtype=tf.float32, name='Area')], ['NumberOfRooms', TensorSpec(shape=(None, 1), dtype=tf.float32, name='NumberOfRooms')], ['Floor', TensorSpec(shape=(None, 1), dtype=tf.float32, name='Floor')], ['Floors', TensorSpec(shape=(None, 1), dtype=tf.float32, name='Floors')], ['BuildingAge', TensorSpec(shape=(None, 1), dtype=tf.float32, name='BuildingAge')], ['Predict_State', TensorSpec(shape=(None, 1), dtype=tf.string, name='Predict_State')], ['Predicted_Loc', TensorSpec(shape=(None, 1), dtype=tf.string, name='Predicted_Loc')], ['BuildingType', TensorSpec(shape=(None, 1), dtype=tf.string, name='BuildingType')], ['TypeOfMarket', TensorSpec(shape=(None, 1), dtype=tf.string, name='TypeOfMarket')], ['Type', TensorSpec(shape=(None, 1), dtype=tf.string, name='Type')], ['OfferFrom', TensorSpec(shape=(None, 1), dtype=tf.string, name='OfferFrom')], ['city_slug', TensorSpec(shape=(None, 1), dtype=tf.string, name='city_slug')], ['text_all', TensorSpec(shape=(None, 1), dtype=tf.string, name='text_all')]]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1893673760720: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1893673751504: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1893673757072: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1893673754384: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1893673303504: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1893673302352: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1893673287952: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1893673293904: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1893673302736: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1893673288720: TensorSpec(shape=(1, 1), dtype=tf.float32, name=None)\n",
      "  1893673296400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1893673297168: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1893673303888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1893673302928: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1893673293520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1893673295440: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1893673296592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1893673295824: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1895573837136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1895573834256: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1895573840016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1895573841744: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1895573838096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1895573838864: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1895573844240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1895573842128: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1893673288336: TensorSpec(shape=(), dtype=tf.string, name=None)\n",
      "  1893673290448: TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
      "  1893673294672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1895573832720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1895573837904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1895573836944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1895573835984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1895573839440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1895573841552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1895573839632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Saved SavedModel at: artifacts_ppm_routing\\regressor__4480766881104448127_sm\n"
     ]
    }
   ],
   "source": [
    "# ==== NN Regressors per group (Price_log_m2) with MC Dropout + Windows-safe saving ====\n",
    "if MODEL_VARIANT in ('NN','BOTH'):\n",
    "    import os, re, numpy as np\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "    def build_regressor(train_subset):\n",
    "        inputs = {}; encoded = []\n",
    "        adapt_ds = tf.data.Dataset.from_tensor_slices(dict(train_subset[REG_INPUTS])).batch(256)\n",
    "        for f in NUMERIC:\n",
    "            inp = keras.Input(shape=(1,), name=f, dtype=tf.float32); inputs[f]=inp\n",
    "            norm = layers.Normalization(axis=-1); norm.adapt(adapt_ds.map(lambda x: tf.expand_dims(tf.cast(x[f], tf.float32),-1)))\n",
    "            encoded.append(norm(inp))\n",
    "        for f in CATEG:\n",
    "            inp = keras.Input(shape=(1,), name=f, dtype=tf.string); inputs[f]=inp\n",
    "            lk = layers.StringLookup(output_mode='one_hot'); lk.adapt(adapt_ds.map(lambda x: x[f]))\n",
    "            encoded.append(lk(inp))\n",
    "        txt = keras.Input(shape=(1,), name='text_all', dtype=tf.string); inputs['text_all']=txt\n",
    "        tv = layers.TextVectorization(max_tokens=TEXT_MAX_TOKENS, output_mode='multi_hot')\n",
    "        tv.adapt(tf.data.Dataset.from_tensor_slices((train_subset['Title'].fillna('')+' '+train_subset['Description'].fillna(''))).batch(256))\n",
    "        encoded.append(tv(txt))\n",
    "        x = layers.Concatenate()(encoded)\n",
    "        x = layers.Dense(256, activation='relu')(x)\n",
    "        x = layers.Dropout(0.3)(x, training=True)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.Dropout(0.3)(x, training=True)\n",
    "        out = layers.Dense(1, name='price_log_m2')(x)\n",
    "        m = keras.Model(inputs, out)\n",
    "        m.compile(optimizer=keras.optimizers.Adam(1e-3), loss='mse', metrics=[keras.metrics.RootMeanSquaredError(name='rmse')])\n",
    "        return m\n",
    "\n",
    "    def df_to_ds(df_):\n",
    "        xdict = {f: df_[f].values for f in NUMERIC+CATEG}\n",
    "        xdict['text_all'] = (df_['Title'].fillna('')+' '+df_['Description'].fillna('')).values\n",
    "        y = df_['Price_log_m2'].astype('float32').values\n",
    "        return tf.data.Dataset.from_tensor_slices((xdict, y)).shuffle(len(df_), seed=SEED).batch(256).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    # ----- Global fallback regressor -----\n",
    "    print('Training global fallback regressor...')\n",
    "    global_reg = build_regressor(train_df)\n",
    "    csvlog = keras.callbacks.CSVLogger(os.path.join(SAVE_DIR,'global_reg_log.csv'))\n",
    "    global_reg.fit(df_to_ds(train_df), validation_data=df_to_ds(val_df), epochs=30, callbacks=[es, csvlog], verbose=1)\n",
    "\n",
    "    # Robust save (.keras -> SavedModel)\n",
    "    base_global = os.path.join(SAVE_DIR,'regressor_global')\n",
    "    try:\n",
    "        global_reg.save(base_global + '.keras')\n",
    "        print('Saved .keras at:', base_global + '.keras')\n",
    "    except UnicodeEncodeError as e:\n",
    "        print('UnicodeEncodeError during .keras save; falling back to SavedModel:', e)\n",
    "        try:\n",
    "            global_reg.export(base_global + '_sm')\n",
    "            print('Saved SavedModel at:', base_global + '_sm')\n",
    "        except Exception:\n",
    "            tf.saved_model.save(global_reg, base_global + '_sm')\n",
    "            print('Saved (legacy) TF SavedModel at:', base_global + '_sm')\n",
    "\n",
    "    # ----- Per-group regressors -----\n",
    "    reg_paths = {}\n",
    "    group_stats = []\n",
    "    for g, gdf in train_df.groupby('GroupLabel'):\n",
    "        if len(gdf) < MIN_SAMPLES_GROUP:\n",
    "            continue\n",
    "        print(f'Training regressor for group {g} (n={len(gdf)})')\n",
    "        reg = build_regressor(gdf)\n",
    "        safe = re.sub(r'[^a-zA-Z0-9_]+', '_', g)\n",
    "        logfile = os.path.join(SAVE_DIR, f'reg_{safe}.csv')\n",
    "        csvlog = keras.callbacks.CSVLogger(logfile)\n",
    "        val_g = val_df[val_df['GroupLabel']==g]\n",
    "        if len(val_g)==0:\n",
    "            val_g = val_df.sample(min(5000, len(val_df)), random_state=SEED)\n",
    "        reg.fit(df_to_ds(gdf), validation_data=df_to_ds(val_g), epochs=20, callbacks=[es, csvlog], verbose=0)\n",
    "\n",
    "        # Robust save per-group\n",
    "        base_group = os.path.join(SAVE_DIR, f'regressor__{abs(hash(g))}')\n",
    "        try:\n",
    "            reg.save(base_group + '.keras')\n",
    "            print('Saved .keras at:', base_group + '.keras')\n",
    "            reg_paths[g] = base_group + '.keras'\n",
    "        except UnicodeEncodeError as e:\n",
    "            print('UnicodeEncodeError during .keras save; falling back to SavedModel:', e)\n",
    "            try:\n",
    "                reg.export(base_group + '_sm')\n",
    "                print('Saved SavedModel at:', base_group + '_sm')\n",
    "                reg_paths[g] = base_group + '_sm'\n",
    "            except Exception:\n",
    "                tf.saved_model.save(reg, base_group + '_sm')\n",
    "                print('Saved (legacy) TF SavedModel at:', base_group + '_sm')\n",
    "                reg_paths[g] = base_group + '_sm'\n",
    "\n",
    "        if len(val_g) >= 50:\n",
    "            xdict = {f: val_g[f].values for f in NUMERIC+CATEG}\n",
    "            xdict['text_all'] = (val_g['Title'].fillna('')+' '+val_g['Description'].fillna('')).values\n",
    "            pred = np.expm1(reg.predict(xdict, verbose=0).ravel())\n",
    "            mape = mean_absolute_percentage_error(val_g['pricepermeter'], pred)\n",
    "            medape = np.median(np.abs((val_g['pricepermeter'].values - pred) / np.clip(val_g['pricepermeter'].values,1e-9,None)))\n",
    "        else:\n",
    "            mape = np.nan; medape = np.nan\n",
    "        group_stats.append({'group':g, 'n_train':len(gdf), 'n_val':len(val_g), 'MAPE':mape, 'MedianAPE':medape})\n",
    "\n",
    "    pd.DataFrame(group_stats).to_csv(os.path.join(SAVE_DIR,'per_group_metrics.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d20df4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009501 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 16842\n",
      "[LightGBM] [Info] Number of data points in the train set: 978929, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 5700.000000\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\ttrain's quantile: 345.752\tvalid's quantile: 349.05\n",
      "[100]\ttrain's quantile: 292.963\tvalid's quantile: 298.88\n",
      "[150]\ttrain's quantile: 273.723\tvalid's quantile: 281.878\n",
      "[200]\ttrain's quantile: 264.42\tvalid's quantile: 274.424\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\ttrain's quantile: 264.42\tvalid's quantile: 274.424\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009386 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 16842\n",
      "[LightGBM] [Info] Number of data points in the train set: 978929, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 10358.471680\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\ttrain's quantile: 760.937\tvalid's quantile: 766.023\n",
      "[100]\ttrain's quantile: 667.075\tvalid's quantile: 677.221\n",
      "[150]\ttrain's quantile: 638.858\tvalid's quantile: 653.579\n",
      "[200]\ttrain's quantile: 624.659\tvalid's quantile: 642.842\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\ttrain's quantile: 624.659\tvalid's quantile: 642.842\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007689 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 16842\n",
      "[LightGBM] [Info] Number of data points in the train set: 978929, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 17836.087891\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\ttrain's quantile: 415.086\tvalid's quantile: 418.999\n",
      "[100]\ttrain's quantile: 346.562\tvalid's quantile: 355.461\n",
      "[150]\ttrain's quantile: 328.817\tvalid's quantile: 340.872\n",
      "[200]\ttrain's quantile: 319.457\tvalid's quantile: 333.913\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\ttrain's quantile: 319.457\tvalid's quantile: 333.913\n",
      "{'GBM_rmse_log_ppm': 0.17799293206893305, 'GBM_MAPE_ppm': 0.1254665039352753, 'GBM_MedianAPE_ppm': 0.08193564143555963}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x1b953c5f990>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== GBM variant: LightGBM quantile (bez OHE, kategorie natywne + early stopping) ====\n",
    "import os, numpy as np, pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# 1) Definicje kolumn\n",
    "cat_cols = ['Predict_State','Predicted_Loc','BuildingType','TypeOfMarket','Type','OfferFrom','city_slug']\n",
    "num_cols = ['Area','NumberOfRooms','Floor','Floors','BuildingAge']\n",
    "\n",
    "# 2) Dtypes kategorii (wymagane przez LightGBM)\n",
    "for c in cat_cols:\n",
    "    train_df[c] = train_df[c].astype('category')\n",
    "    val_df[c]   = val_df[c].astype('category')\n",
    "\n",
    "# 3) Macierze wejściowe (bez TF‑IDF, by przyspieszyć i obniżyć RAM)\n",
    "Xtr = train_df[num_cols + cat_cols]\n",
    "ytr = train_df['pricepermeter'].values.astype('float32')\n",
    "Xva = val_df[num_cols + cat_cols]\n",
    "yva = val_df['pricepermeter'].values.astype('float32')\n",
    "\n",
    "# 4) Datasets z informacją o kolumnach kategorycznych\n",
    "dtrain = lgb.Dataset(Xtr, label=ytr, categorical_feature=cat_cols, free_raw_data=False)\n",
    "dvalid = lgb.Dataset(Xva, label=yva, categorical_feature=cat_cols, free_raw_data=False)\n",
    "\n",
    "# 5) Funkcja ucząca kwantyl (używa early stopping i 32 wątków)\n",
    "def fit_lgb_quantile(alpha, seed=SEED):\n",
    "    params = {\n",
    "        'objective': 'quantile',\n",
    "        'alpha': alpha,\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 64,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 1,\n",
    "        'min_data_in_leaf': 50,\n",
    "        'seed': int(seed),\n",
    "        'num_threads': 32\n",
    "    }\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=200,\n",
    "        valid_sets=[dtrain, dvalid],\n",
    "        valid_names=['train','valid'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=30, first_metric_only=False, verbose=True),\n",
    "            lgb.log_evaluation(period=50)\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# 6) Trening trzech modeli kwantylowych\n",
    "lgb_p10 = fit_lgb_quantile(0.10)\n",
    "lgb_p50 = fit_lgb_quantile(0.50)\n",
    "lgb_p90 = fit_lgb_quantile(0.90)\n",
    "\n",
    "# 7) Predykcje z najlepszej iteracji\n",
    "p10 = lgb_p10.predict(Xva, num_iteration=lgb_p10.best_iteration)\n",
    "p50 = lgb_p50.predict(Xva, num_iteration=lgb_p50.best_iteration)\n",
    "p90 = lgb_p90.predict(Xva, num_iteration=lgb_p90.best_iteration)\n",
    "\n",
    "# 8) Kalibracja prawdopodobieństwa trafienia pasma ±ALPHA_PCT wokół mediany\n",
    "low  = p50 * (1 - ALPHA_PCT)\n",
    "high = p50 * (1 + ALPHA_PCT)\n",
    "event = ((yva >= low) & (yva <= high)).astype(int)\n",
    "\n",
    "# Surowy „szerokość/poziom” pasma z modeli kwantylowych\n",
    "raw_width_cov = ((p90 - p10) / np.maximum(1.0, p50)).clip(0, 1)\n",
    "\n",
    "iso = IsotonicRegression(out_of_bounds='clip').fit(raw_width_cov, event)\n",
    "prob = iso.predict(raw_width_cov)\n",
    "\n",
    "# 9) Metryki walidacyjne\n",
    "rmse_log = float(np.sqrt(np.mean((np.log1p(yva) - np.log1p(p50))**2)))\n",
    "mape = float(mean_absolute_percentage_error(yva, p50))\n",
    "medape = float(np.median(np.abs((yva - p50) / np.clip(yva, 1e-9, None))))\n",
    "print({'GBM_rmse_log_ppm': rmse_log, 'GBM_MAPE_ppm': mape, 'GBM_MedianAPE_ppm': medape})\n",
    "\n",
    "# 10) Próbka 20 i zapis modeli\n",
    "rng = np.random.RandomState(SEED)\n",
    "sample_idx = rng.choice(len(val_df), size=min(20, len(val_df)), replace=False)\n",
    "sample20 = val_df.iloc[sample_idx][['SaleId','Title','pricepermeter','Area','Predicted_Loc']].copy()\n",
    "sample20['Predicted_ppm'] = p50[sample_idx]\n",
    "sample20['Probability']   = prob[sample_idx]\n",
    "sample20['GroupLabel_pred'] = val_df.iloc[sample_idx]['GroupLabel'].values\n",
    "sample20['ClassConfidence']  = 1.0\n",
    "sample20 = sample20[['SaleId','Title','pricepermeter','Predicted_ppm','Probability','GroupLabel_pred','ClassConfidence','Area','Predicted_Loc']]\n",
    "sample20['Predicted_ppm'] = sample20['Predicted_ppm'].round(0)\n",
    "sample20['Probability']   = sample20['Probability'].round(3)\n",
    "sample20.to_csv(os.path.join(SAVE_DIR,'predictions_sample20_GBM.csv'), index=False)\n",
    "\n",
    "# Modele (tekstowe pliki .txt są bezpieczne pod Windows)\n",
    "lgb_p10.save_model(os.path.join(SAVE_DIR,'lgb_q10.txt'))\n",
    "lgb_p50.save_model(os.path.join(SAVE_DIR,'lgb_q50.txt'))\n",
    "lgb_p90.save_model(os.path.join(SAVE_DIR,'lgb_q90.txt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97a12a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call evaluate_correctness(sample_df) to compute trafność and corrections.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== Price correctness & correction on ppm scale ====\n",
    "# For any predictions DataFrame with columns: 'pricepermeter', 'Predicted_ppm', 'Probability', and 'Area'\n",
    "\n",
    "def evaluate_correctness(df_pred, alpha_pct=ALPHA_PCT, prob_threshold=0.5):\n",
    "    import numpy as np\n",
    "    dfp = df_pred.copy()\n",
    "    dfp['ppm_bs'] = dfp['pricepermeter']\n",
    "    dfp['ppm_pred'] = dfp['Predicted_ppm']\n",
    "    dfp['Delta_ppm'] = dfp['ppm_pred'] - dfp['ppm_bs']\n",
    "    dfp['Delta_total'] = dfp['Delta_ppm'] * dfp['Area']\n",
    "    dfp['is_trafiona'] = (np.abs(dfp['Delta_ppm']) <= alpha_pct * dfp['ppm_pred']) & (dfp['Probability'] >= prob_threshold)\n",
    "    return dfp\n",
    "\n",
    "print('Call evaluate_correctness(sample_df) to compute trafność and corrections.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68b4c204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts saved to artifacts_ppm_routing\n"
     ]
    }
   ],
   "source": [
    "# ==== Save config and minimal README ====\n",
    "import os, json\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(SAVE_DIR, 'config.json'), 'w', encoding='utf-8') as f:\n",
    "    json.dump({\n",
    "        'MODEL_VARIANT': MODEL_VARIANT,\n",
    "        'N_BINS': N_BINS,\n",
    "        'MIN_SAMPLES_GROUP': MIN_SAMPLES_GROUP,\n",
    "        'GEO_MODE': GEO_MODE,\n",
    "        'H3_RESOLUTION': H3_RESOLUTION,\n",
    "        'ALPHA_PCT': ALPHA_PCT,\n",
    "        'SEED': SEED\n",
    "    }, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(os.path.join(SAVE_DIR, 'README.txt'), 'w', encoding='utf-8') as f:\n",
    "    f.write('Artifacts saved for hedonic ppm routing experiment.\\n')\n",
    "\n",
    "print('Artifacts saved to', SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc64c6a0-bd16-4866-93e3-bca7f976cd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used: GBM_q50_ppm\n",
      "          SaleId                                              Title  \\\n",
      "420899   2735273  Nowe mieszkanie trzypokojowe(NrA_56) Unii Lube...   \n",
      "1298242  4987862  Mieszkanie 3-pokojowe umeblowane i gotowe do z...   \n",
      "575917   3052776              Mieszkanie, Sosnowiec, Zagórze, 62 m²   \n",
      "1151407  4599949     3pok mieszkanie z ogródkiem poniżej 500tyś.!!!   \n",
      "476638   2847992  Piękne 3-pokojowe mieszkanie Armii Krajowej Ol...   \n",
      "464882   2824189                   SPRZEDAM MIESZKANIE BEZCZYNSZOWE   \n",
      "1317549  5036565                 Mieszkanie 33m2, parter z balkonem   \n",
      "1024795  4255505             Mieszkanie, Gdańsk, Śródmieście, 41 m²   \n",
      "1256606  4885755                        Mieszkanie, Warszawa, 57 m²   \n",
      "766454   3554140                  Mieszkanie, Ustroń, Ustroń, 48 m²   \n",
      "892894   3897185  Nowe Południe - mieszkanie 5.A.04 - Nowa ofert...   \n",
      "897700   3911507  Kawalerka w Śródmieściu, po Remoncie, Niski Cz...   \n",
      "1091555  4436620                           Mieszkanie, ul. Akacjowa   \n",
      "1416       10631                 Sprzedam/M3/Śródmieście/Do remontu   \n",
      "193954   1415497             Mieszkanie, Kraków, Zwierzyniec, 73 m²   \n",
      "918957   3970359                             Mieszkanie, ul. Ryżowa   \n",
      "574203   3048817     PROMOCJA | ❗Apartament w Nowym Centrum Łodzi!❗   \n",
      "725930   3438547       Osiedle Dębowy Park | mieszkanie 3-pok. | G1   \n",
      "1135689  4545158               3-pokojowe mieszkanie 68m2 + ogródek   \n",
      "12005      83028          Mega kawalerka w bloku, zamknięte osiedle   \n",
      "\n",
      "                                             Predicted_Loc     Predict_State  \\\n",
      "420899   Poznań -> Poznań-nowe miasto -> Starołęka mała...   DEVELOPER_STATE   \n",
      "1298242                          Krapkowice -> ? -> ? -> ?  AFTER_RENOVATION   \n",
      "575917                            Sosnowiec -> ? -> ? -> ?    FOR_RENOVATION   \n",
      "1151407                              Gdynia -> ? -> ? -> ?   DEVELOPER_STATE   \n",
      "476638                               Olkusz -> ? -> ? -> ?  AFTER_RENOVATION   \n",
      "464882                                    ? -> ? -> ? -> ?    FOR_RENOVATION   \n",
      "1317549                               Konin -> ? -> ? -> ?              GOOD   \n",
      "1024795                              Gdańsk -> ? -> ? -> ?   DEVELOPER_STATE   \n",
      "1256606                            Warszawa -> ? -> ? -> ?              GOOD   \n",
      "766454                               Ustroń -> ? -> ? -> ?   DEVELOPER_STATE   \n",
      "892894            Gdańsk -> ? -> Łostowice -> Starogardzka   DEVELOPER_STATE   \n",
      "897700                                Opole -> ? -> ? -> ?  AFTER_RENOVATION   \n",
      "1091555                               Gądki -> ? -> ? -> ?              GOOD   \n",
      "1416                            Częstochowa -> ? -> ? -> ?    FOR_RENOVATION   \n",
      "193954        Kraków -> Kraków-krowodrza -> Krowodrza -> ?   DEVELOPER_STATE   \n",
      "918957                         Warszawa -> Ursus -> ? -> ?              GOOD   \n",
      "574203                  Łódź -> Łódź-widzew -> Widzew -> ?   DEVELOPER_STATE   \n",
      "725930   Siemianowice śląskie -> ? -> Michałkowice -> R...   DEVELOPER_STATE   \n",
      "1135689  Kraków -> Kraków-krowodrza -> Prądnik biały ->...   DEVELOPER_STATE   \n",
      "12005                   Łódź -> Łódź-bałuty -> Bałuty -> ?              GOOD   \n",
      "\n",
      "               Price  Predicted_Price              Group_Assigned  \n",
      "420899    877,386.00       796,496.22                poznań__bin6  \n",
      "1298242   345,000.00       353,530.96            krapkowice__bin0  \n",
      "575917    389,000.00       332,963.16             sosnowiec__bin2  \n",
      "1151407   498,000.00       560,595.15                gdynia__bin1  \n",
      "476638    458,000.00       385,959.31                olkusz__bin0  \n",
      "464882    255,000.00       271,633.43               unknown__bin1  \n",
      "1317549   230,000.00       220,549.39                 konin__bin7  \n",
      "1024795   650,000.00       573,727.85                gdańsk__bin7  \n",
      "1256606   780,000.00       862,038.48              warszawa__bin1  \n",
      "766454    540,176.00       559,098.22                ustroń__bin0  \n",
      "892894    511,000.00       522,761.66                gdańsk__bin1  \n",
      "897700    315,000.00       334,246.78                 opole__bin6  \n",
      "1091555   549,000.00       544,355.33                 gądki__bin0  \n",
      "1416      280,000.00       271,870.47           częstochowa__bin1  \n",
      "193954  1,377,690.00     1,193,003.16                kraków__bin8  \n",
      "918957  1,227,000.00     1,163,233.97              warszawa__bin2  \n",
      "574203    395,500.00       446,885.11                  łódź__bin6  \n",
      "725930    605,640.00       595,700.27  siemianowice_śląskie__bin8  \n",
      "1135689   938,713.00       947,475.02                kraków__bin2  \n",
      "12005     249,000.00       205,448.69                  łódź__bin8  \n",
      "Saved full predicted base to: Predicted_base_full.csv with shape: (1223662, 24)\n"
     ]
    }
   ],
   "source": [
    "# ==== Apply trained model(s) to full base and save Predicted_base_full.csv ====\n",
    "import os, numpy as np, pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "# 1) Wybór pełnej bazy (po preprocessie)\n",
    "# Jeśli 'df' istnieje, to jest to pełna baza po przygotowaniu; w innym razie sklej train/val.\n",
    "if 'df' in globals():\n",
    "    base_df = df.copy()\n",
    "else:\n",
    "    base_df = pd.concat([train_df, val_df], axis=0, ignore_index=True)\n",
    "\n",
    "# Zachowaj kopię do zapisu\n",
    "pred_df = base_df.copy()\n",
    "\n",
    "# 2) Domyślna kolumna grupy (jeśli brak predykcji klasą, użyj GroupLabel z binningu)\n",
    "pred_df['Group_Assigned'] = pred_df.get('_Group_pred', pred_df.get('GroupLabel', 'unknown')).astype(str)\n",
    "\n",
    "# 3) Spróbuj użyć wariantu GBM (median - q50), jeśli plik istnieje\n",
    "save_dir = SAVE_DIR if 'SAVE_DIR' in globals() else 'artifacts_ppm_routing'\n",
    "model_q50_path = os.path.join(save_dir, 'lgb_q50.txt')\n",
    "\n",
    "used_model = None\n",
    "if os.path.exists(model_q50_path):\n",
    "    # Przygotuj typy kategorii tak jak w treningu GBM\n",
    "    cat_cols = ['Predict_State','Predicted_Loc','BuildingType','TypeOfMarket','Type','OfferFrom','city_slug']\n",
    "    num_cols = ['Area','NumberOfRooms','Floor','Floors','BuildingAge']\n",
    "    for c in cat_cols:\n",
    "        pred_df[c] = pred_df[c].astype('category')\n",
    "    Xall = pred_df[num_cols + cat_cols]\n",
    "    booster = lgb.Booster(model_file=model_q50_path)\n",
    "    p50_all = booster.predict(Xall, num_iteration=getattr(booster, 'best_iteration', None))\n",
    "    pred_df['Predicted_ppm'] = p50_all\n",
    "    pred_df['Predicted_Price'] = pred_df['Predicted_ppm'] * pred_df['Area']\n",
    "    used_model = 'GBM_q50_ppm'\n",
    "else:\n",
    "    # Fallback: jeśli zapisano globalny regressor Keras, wykorzystaj go (ppm na log-skali)\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    base_global = os.path.join(save_dir,'regressor_global')\n",
    "    path_keras = base_global + '.keras'\n",
    "    path_sm = base_global + '_sm'\n",
    "    model = None\n",
    "    if os.path.exists(path_keras):\n",
    "        model = keras.models.load_model(path_keras)\n",
    "    elif os.path.exists(path_sm):\n",
    "        try:\n",
    "            model = keras.models.load_model(path_sm)\n",
    "        except Exception:\n",
    "            model = tf.saved_model.load(path_sm)\n",
    "    if model is None:\n",
    "        raise RuntimeError('Nie znaleziono modelu do predykcji: brak lgb_q50.txt oraz regressor_global.* w SAVE_DIR')\n",
    "    # Zbuduj wejście tak jak w treningu regresora (NUMERIC+CATEG+text_all)\n",
    "    NUMERIC = ['Area','NumberOfRooms','Floor','Floors','BuildingAge']\n",
    "    CATEG = ['Predict_State','Predicted_Loc','BuildingType','TypeOfMarket','Type','OfferFrom','city_slug']\n",
    "    xdict = {f: pred_df[f].values for f in NUMERIC+CATEG}\n",
    "    xdict['text_all'] = (pred_df['Title'].fillna('') + ' ' + pred_df['Description'].fillna('')).values\n",
    "    ylog = model.predict(xdict, batch_size=512, verbose=1).ravel()\n",
    "    pred_df['Predicted_ppm'] = np.expm1(ylog)\n",
    "    pred_df['Predicted_Price'] = pred_df['Predicted_ppm'] * pred_df['Area']\n",
    "    used_model = 'NN_global_ppm'\n",
    "\n",
    "# 4) Rzutowania i zabezpieczenia\n",
    "pred_df['Predicted_Price'] = pred_df['Predicted_Price'].astype(float)\n",
    "\n",
    "# 5) Próbka 20 losowych rekordów do podglądu\n",
    "SEED = SEED if 'SEED' in globals() else 42\n",
    "sample_cols = ['SaleId','Title','Predicted_Loc','Predict_State','Price','Predicted_Price','Group_Assigned']\n",
    "sample_cols = [c for c in sample_cols if c in pred_df.columns]\n",
    "sample20 = pred_df.sample(n=min(20, len(pred_df)), random_state=SEED)[sample_cols].copy()\n",
    "print('Model used:', used_model)\n",
    "print(sample20.head(20))\n",
    "\n",
    "# 6) Zapis całej bazy z predykcją\n",
    "out_path = 'Predicted_base_full.csv'\n",
    "pred_df.to_csv(out_path, sep=';', encoding='utf-8-sig', index=False)\n",
    "print('Saved full predicted base to:', out_path, 'with shape:', pred_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e8e53e5-7831-46df-afe2-36ae4f4e2ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     SaleId                                              Title  \\\n",
      "0   3847651                            Mieszkanie, ul. Zamkowa   \n",
      "1   3920583    Mieszkanie w centrum Jawor 2 pokoje po remoncie   \n",
      "2   2797305  2pok 41met, okolice Sztabowej LOGGIA/PIWNICA/G...   \n",
      "3   5052646         Widok na cały Kraków - Górka Narodowa 60m2   \n",
      "4   3210230                               Mieszkanie Bydgoszcz   \n",
      "5   5109215                             Mieszkanie, ul. Smolna   \n",
      "6   3523743     Kraków – Podgórze – ul. Krasickiego – 37,71 m²   \n",
      "7   3861971                               Mieszkanie Sosnowiec   \n",
      "8    421056    Promocja 2 pokoje Sosnowiec Sielec Klimontowska   \n",
      "9   2014107                          Mieszkanie Świętochłowice   \n",
      "10  3491630                    Przytulne 3 pokoje z ogrodem L7   \n",
      "11  2844584  LUNA Gdańsk Galaktyczna dwupoziomowe 58 m2 do ...   \n",
      "12  4701335    Narożne: wyjątkowy rozkład 2 pokoi - dwustronne   \n",
      "13  3291881                    Mieszkanie, Łódź, Bałuty, 37 m²   \n",
      "14  2890421        Ostatnie piętro | TRAMWAJ | Park na Zdrowiu   \n",
      "\n",
      "                                       Predicted_Loc     Predict_State  \\\n",
      "0                            Katowice -> ? -> ? -> ?    FOR_RENOVATION   \n",
      "1                               Jawor -> ? -> ? -> ?    FOR_RENOVATION   \n",
      "2             Wrocław -> Wrocław-fabryczna -> ? -> ?              GOOD   \n",
      "3   Kraków -> Kraków-krowodrza -> Prądnik biały -> ?              GOOD   \n",
      "4                           Bydgoszcz -> ? -> ? -> ?              GOOD   \n",
      "5        Poznań -> Poznań-nowe miasto -> Rataje -> ?              GOOD   \n",
      "6                Kraków -> Kraków-podgórze -> ? -> ?    FOR_RENOVATION   \n",
      "7                           Sosnowiec -> ? -> ? -> ?              GOOD   \n",
      "8                           Sosnowiec -> ? -> ? -> ?   DEVELOPER_STATE   \n",
      "9                      Świętochłowice -> ? -> ? -> ?  AFTER_RENOVATION   \n",
      "10                 Goczałkowice-zdrój -> ? -> ? -> ?   DEVELOPER_STATE   \n",
      "11                             Gdańsk -> ? -> ? -> ?   DEVELOPER_STATE   \n",
      "12           Warszawa -> Białołęka -> Nowodwory -> ?   DEVELOPER_STATE   \n",
      "13                Łódź -> Łódź-bałuty -> Bałuty -> ?    FOR_RENOVATION   \n",
      "14              Łódź -> Łódź-polesie -> Polesie -> ?   DEVELOPER_STATE   \n",
      "\n",
      "          Price  Predicted_Price                    Group  \n",
      "0    219,000.00       260,431.70           katowice__bin0  \n",
      "1    219,000.00       233,802.22              jawor__bin0  \n",
      "2    549,000.00       530,835.22            wrocław__bin5  \n",
      "3  1,000,000.00       953,267.50             kraków__bin6  \n",
      "4    350,000.00       333,384.96          bydgoszcz__bin1  \n",
      "5    509,000.00       408,567.55             poznań__bin6  \n",
      "6    549,000.00       527,749.28             kraków__bin3  \n",
      "7    329,000.00       317,124.97          sosnowiec__bin2  \n",
      "8    399,000.00       337,049.78          sosnowiec__bin9  \n",
      "9    135,000.00       206,925.22     świętochłowice__bin0  \n",
      "10   518,252.00       541,064.24  goczałkowicezdrój__bin0  \n",
      "11   599,000.00       592,873.14             gdańsk__bin1  \n",
      "12   592,380.00       589,881.11           warszawa__bin1  \n",
      "13   510,000.00       424,532.94               łódź__bin9  \n",
      "14   289,000.00       290,402.83               łódź__bin7  \n"
     ]
    }
   ],
   "source": [
    "# ==== Show fixed examples by SaleId (same as v10 sample) ====\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "# Ensure previous prediction cell has been run to create pred_df\n",
    "if 'pred_df' not in globals():\n",
    "    raise RuntimeError('Brak ramki pred_df. Najpierw uruchom komórkę z inferencją i zapisaniem Predicted_base_full.csv.')\n",
    "\n",
    "# SaleId list taken from the v10 sample table\n",
    "sale_ids = [\n",
    "    3847651, 3920583, 2797305, 5052646, 3210230,\n",
    "    5109215, 3523743, 3861971, 421056, 2014107,\n",
    "    3491630, 2844584, 4701335, 3291881, 2890421\n",
    "]\n",
    "\n",
    "# Normalize SaleId to numeric for matching\n",
    "pred_df['_SaleId_num'] = pd.to_numeric(pred_df['SaleId'], errors='coerce')\n",
    "mask = pred_df['_SaleId_num'].isin(sale_ids)\n",
    "subset = pred_df.loc[mask].copy()\n",
    "\n",
    "# Preserve the given order\n",
    "order_map = {sid: i for i, sid in enumerate(sale_ids)}\n",
    "subset['_order'] = subset['_SaleId_num'].map(order_map)\n",
    "subset = subset.sort_values('_order')\n",
    "\n",
    "# Select and rename columns for display\n",
    "cols = ['SaleId','Title','Predicted_Loc','Predict_State','Price','Predicted_Price','Group_Assigned']\n",
    "cols = [c for c in cols if c in subset.columns]\n",
    "view = subset[cols].copy()\n",
    "view = view.rename(columns={'Group_Assigned':'Group'})\n",
    "\n",
    "# Report missing ids, if any\n",
    "found = set(subset['_SaleId_num'].dropna().astype(int).tolist())\n",
    "missing = [sid for sid in sale_ids if sid not in found]\n",
    "if missing:\n",
    "    print('Uwaga: Nie znaleziono następujących SaleId w pred_df:', missing)\n",
    "\n",
    "print(view.reset_index(drop=True).head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1ac6f3-9485-4c43-80aa-f7ea5bbfdb79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
