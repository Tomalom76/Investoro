{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "028e9be3-0fda-4222-a4a8-e66024588a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/16 16:48:27 INFO mlflow.tracking.fluent: Experiment with name 'Warsaw_Street_Prediction_FINAL' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow ustawiony. Eksperyment: 'Warsaw_Street_Prediction_FINAL'\n",
      "\n",
      "Wczytywanie danych...\n",
      "Wczytano dane główne: (196240, 51), Słownik ulic: (293077, 9)\n"
     ]
    }
   ],
   "source": [
    "# === SEKCJA 1: IMPORT I KONFIGURACJA ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pycaret.classification import setup, compare_models, tune_model, finalize_model, save_model, pull\n",
    "from IPython.display import display\n",
    "\n",
    "MLFLOW_EXPERIMENT_NAME = 'Warsaw_Street_Prediction_FINAL'\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "print(f\"MLflow ustawiony. Eksperyment: '{MLFLOW_EXPERIMENT_NAME}'\")\n",
    "\n",
    "print(\"\\nWczytywanie danych...\")\n",
    "df_main = pd.read_csv('data.csv', sep=',')\n",
    "df_ulic = pd.read_csv('Ulic.csv', sep=',') # Wczytujemy poprawny plik z ulicami\n",
    "print(f\"Wczytano dane główne: {df_main.shape}, Słownik ulic: {df_ulic.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be22b3c8-8259-4ff7-a3b7-787dcffb27df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_pom_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_pom_1\u001b[38;5;241m.\u001b[39minfo()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_pom_1' is not defined"
     ]
    }
   ],
   "source": [
    "df_pom_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae820fe-ca29-436a-97b2-9c3a79c2b7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pom_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3391e185-ccb5-4e35-830a-f80c57fce302",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "['SYM_UL', 'NAZWA_1']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12684\\712248825.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# === SEKCJA 2: PRZYGOTOWANIE I WZBOGACANIE DANYCH ===\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# --- Krok 2a: Stworzenie słownika ulic z pliku Ulic.csv ---\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Kluczem będzie symbol ulicy (SYM_UL), wartością jej pełna nazwa.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdf_ulic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SYM_UL'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NAZWA_1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mstreet_dictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_ulic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNAZWA_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_ulic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSYM_UL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Utworzono słownik z {len(street_dictionary)} unikalnymi ulicami.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\projekt1\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001b[0m\n\u001b[0;32m   6414\u001b[0m             \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6415\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6416\u001b[0m             \u001b[0mcheck\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6417\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6418\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6419\u001b[0m             \u001b[0magg_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6421\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mthresh\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_default\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ['SYM_UL', 'NAZWA_1']"
     ]
    }
   ],
   "source": [
    "# === SEKCJA 2: PRZYGOTOWANIE I WZBOGACANIE DANYCH ===\n",
    "\n",
    "# --- Krok 2a: Stworzenie słownika ulic z pliku Ulic.csv ---\n",
    "# Kluczem będzie symbol ulicy (SYM_UL), wartością jej pełna nazwa.\n",
    "df_ulic.dropna(subset=['SYM_UL', 'NAZWA_1'], inplace=True)\n",
    "street_dictionary = pd.Series(df_ulic.NAZWA_1.values, index=df_ulic.SYM_UL).to_dict()\n",
    "print(f\"Utworzono słownik z {len(street_dictionary)} unikalnymi ulicami.\")\n",
    "\n",
    "# --- Krok 2b: Czyszczenie i inżynieria cech ---\n",
    "df_processed = df_main.copy()\n",
    "print(f\"\\nRozmiar początkowy: {df_processed.shape}\")\n",
    "\n",
    "# Podstawowe czyszczenie\n",
    "df_processed.dropna(subset=['Area', 'Price', 'Location', 'Description'], inplace=True)\n",
    "print(f\"Rozmiar po usunięciu NaN: {df_processed.shape}\")\n",
    "\n",
    "# --- Krok 2c: Inteligentna ekstrakcja Dzielnicy i Ulicy ---\n",
    "def process_location(row):\n",
    "    location_str = row['Location']\n",
    "    if not isinstance(location_str, str):\n",
    "        return pd.Series([np.nan, np.nan])\n",
    "\n",
    "    parts = [p.strip() for p in location_str.split(',')]\n",
    "    district = np.nan\n",
    "    street = np.nan\n",
    "\n",
    "    # Ekstrakcja dzielnicy\n",
    "    if len(parts) >= 3:\n",
    "        district = parts[2]\n",
    "    \n",
    "    # Ekstrakcja ulicy (priorytet dla słownika TERYT)\n",
    "    if pd.notna(row['StreetNumber']):\n",
    "        try:\n",
    "            street_sym = int(row['StreetNumber'])\n",
    "            if street_sym in street_dictionary:\n",
    "                street = street_dictionary[street_sym]\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "    \n",
    "    # Jeśli słownik zawiódł, próbuj z tekstu\n",
    "    if pd.isna(street) and len(parts) >= 4:\n",
    "        street = parts[3]\n",
    "    \n",
    "    # Czyszczenie nazwy ulicy\n",
    "    if isinstance(street, str):\n",
    "        street = re.sub(r'^(ul\\.|al\\.|al\\.|pl\\.)\\s*', '', street, flags=re.IGNORECASE).lower()\n",
    "        if len(street) < 3: street = np.nan # Odrzuć bardzo krótkie nazwy\n",
    "            \n",
    "    return pd.Series([district, street])\n",
    "\n",
    "# Zastosuj funkcję i stwórz nowe kolumny\n",
    "df_processed[['District', 'Ulica_clean']] = df_processed.apply(process_location, axis=1)\n",
    "\n",
    "# Usuń wiersze, gdzie nie udało się zidentyfikować ulicy\n",
    "df_processed.dropna(subset=['Ulica_clean'], inplace=True)\n",
    "print(f\"Po ekstrakcji i czyszczeniu ulic -> Pozostało wierszy: {len(df_processed)}, Unikalnych ulic: {df_processed['Ulica_clean'].nunique()}\")\n",
    "\n",
    "# --- Krok 2d: Ostatnie przygotowania ---\n",
    "# Konwersja daty\n",
    "df_processed['BuiltYear'] = pd.to_datetime(df_processed['BuiltYear'], format='%Y', errors='coerce')\n",
    "\n",
    "# Usunięcie outlierów (teraz, na czystym zbiorze)\n",
    "q_low = df_processed[\"Price\"].quantile(0.01)\n",
    "q_hi  = df_processed[\"Price\"].quantile(0.99)\n",
    "df_processed = df_processed[(df_processed[\"Price\"] < q_hi) & (df_processed[\"Price\"] > q_low)]\n",
    "print(f\"Po usunięciu outlierów cenowych -> Pozostało wierszy: {len(df_processed)}\")\n",
    "\n",
    "df_final = df_processed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f582043e-5d87-489c-ad6e-7e21db77694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEKCJA 3: OSTATECZNE PRZYGOTOWANIE I SETUP ===\n",
    "\n",
    "# --- Krok 3a: Filtrowanie rzadkich ULIC ---\n",
    "MIN_SAMPLES_PER_STREET = 15 # Zaczynamy z solidnym progiem\n",
    "street_counts = df_final['Ulica_clean'].value_counts()\n",
    "streets_to_remove = street_counts[street_counts < MIN_SAMPLES_PER_STREET].index\n",
    "df_model_ready = df_final[~df_final['Ulica_clean'].isin(streets_to_remove)].copy()\n",
    "print(f\"Po odfiltrowaniu rzadkich ulic -> Pozostało wierszy: {len(df_model_ready)}, Unikalnych ulic: {df_model_ready['Ulica_clean'].nunique()}\")\n",
    "\n",
    "# --- Krok 3b: Podział na zbiory i przygotowanie grup ---\n",
    "data_train = df_model_ready.sample(frac=0.9, random_state=1122)\n",
    "data_test = df_model_ready.drop(data_train.index)\n",
    "fold_groups = data_train['Ulica_clean'] # Do stabilnej walidacji\n",
    "print(f\"Podział danych -> Treningowe: {data_train.shape}, Testowe: {data_test.shape}\")\n",
    "\n",
    "# --- Krok 3c: Setup PyCaret ---\n",
    "# Definicja cech. Teraz dołączamy dzielnicę jako predyktor!\n",
    "categorical_features = ['District', 'BuildingType', 'TypeOfMarket']\n",
    "# PyCaret sam sobie poradzi z resztą (numeryczne, daty)\n",
    "\n",
    "s = setup(\n",
    "    data=data_train,\n",
    "    test_data=data_test,\n",
    "    target='Ulica_clean',\n",
    "    session_id=1122,\n",
    "    n_jobs=4,\n",
    "    \n",
    "    # Używamy stabilizacji przez grupowanie\n",
    "    fold_strategy='groupkfold',\n",
    "    fold_groups=fold_groups,\n",
    "    \n",
    "    # Jawnie definiujemy tylko najważniejsze cechy kategoryczne\n",
    "    categorical_features=categorical_features,\n",
    "    \n",
    "    # Ignorujemy wszystko, co jest ID lub surowym tekstem\n",
    "    ignore_features=['SaleId', 'OriginalId', 'Location', 'Description', 'Title', 'Link'],\n",
    "    \n",
    "    # Zabezpieczenia\n",
    "    log_experiment=False,\n",
    "    html=False,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36e317b-e294-4370-8600-3ef699a86fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEKCJA 4: PRZYGOTOWANIE DO MODELOWANIA (ULEPSZONE) ===\n",
    "\n",
    "# --- Krok 4a: Bardziej Agresywne Filtrowanie ---\n",
    "MIN_SAMPLES_PER_STREET = 15 # Zwiększamy próg, aby zapewnić stabilność\n",
    "street_counts = df_final['Ulica_clean'].value_counts()\n",
    "streets_to_remove = street_counts[street_counts < MIN_SAMPLES_PER_STREET].index\n",
    "df_model_ready = df_final[~df_final['Ulica_clean'].isin(streets_to_remove)].copy()\n",
    "\n",
    "print(f\"Użyto progu MIN_SAMPLES_PER_STREET = {MIN_SAMPLES_PER_STREET}\")\n",
    "print(f\"Usunięto {len(streets_to_remove)} rzadkich ulic. Pozostało {len(df_model_ready)} wierszy.\")\n",
    "print(f\"Liczba klas (ulic) do predykcji: {df_model_ready['Ulica_clean'].nunique()}\")\n",
    "\n",
    "# --- Krok 4b: Podział na zbiór treningowy i testowy ---\n",
    "data_train = df_model_ready.sample(frac=0.9, random_state=1122)\n",
    "data_test = df_model_ready.drop(data_train.index)\n",
    "print(f\"\\nPodział danych -> Treningowe: {data_train.shape}, Testowe: {data_test.shape}\")\n",
    "\n",
    "# --- Krok 4c (NOWOŚĆ): Przygotowanie grup do walidacji krzyżowej ---\n",
    "# Tworzymy kolumnę, która będzie używana do grupowania foldów.\n",
    "# Dzięki temu wszystkie wiersze z tą samą ulicą trafią zawsze do tego samego foldu.\n",
    "fold_groups = data_train['Ulica_clean']\n",
    "print(\"\\nPrzygotowano grupy do stabilnej walidacji krzyżowej.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e878ad-055d-4c65-b43e-42862c3a1ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEKCJA 5: SETUP I TRENING (ULEPSZONE) ===\n",
    "\n",
    "# Definicje list cech (bez zmian)\n",
    "# ... (kopiuj kod z definicjami numeric_features, categorical_features, etc. z poprzedniej wersji)\n",
    "numeric_features = [c for c in data_train.columns if c.startswith('des_tfidf_')] + ['Area', 'Price', 'NumberOfRooms', 'Floor', 'Floors']\n",
    "categorical_features = ['BuildingType', 'TypeOfMarket', 'District'] \n",
    "date_features = ['BuiltYear']\n",
    "numeric_features = [f for f in numeric_features if f in data_train.columns]\n",
    "categorical_features = [f for f in categorical_features if f in data_train.columns]\n",
    "date_features = [f for f in date_features if f in data_train.columns]\n",
    "if 'BuiltYear' in date_features:\n",
    "    data_train['BuiltYear'] = pd.to_datetime(data_train['BuiltYear'], errors='coerce')\n",
    "    data_test['BuiltYear'] = pd.to_datetime(data_test['BuiltYear'], errors='coerce')\n",
    "for col in categorical_features:\n",
    "    data_train[col] = data_train[col].astype(str).fillna('missing')\n",
    "    data_test[col] = data_test[col].astype(str).fillna('missing')\n",
    "ignore_features = [col for col in ['SaleId', 'OriginalId', 'Location', 'Description', 'Title', 'Link'] if col in data_train.columns]\n",
    "\n",
    "\n",
    "# Setup\n",
    "s = setup(\n",
    "    data=data_train,\n",
    "    test_data=data_test,\n",
    "    target='Ulica_clean',\n",
    "    session_id=1122,\n",
    "    log_experiment=False,\n",
    "    n_jobs=4,\n",
    "    \n",
    "    # === KLUCZOWA ZMIANA: Używamy grupowania, aby ustabilizować walidację ===\n",
    "    fold_strategy='groupkfold',\n",
    "    fold_groups=fold_groups,\n",
    "    \n",
    "    numeric_features=numeric_features,\n",
    "    categorical_features=categorical_features,\n",
    "    date_features=date_features,\n",
    "    ignore_features=ignore_features,\n",
    "    \n",
    "    html=False,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be15ef4d-c830-4ecf-a436-c3fa8ace53c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEKCJA 6: TRENING I LOGOWANIE DO MLFLOW (ULEPSZONE) ===\n",
    "\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run(run_name=\"Warsaw_Street_Prediction_Run_v2\") as run:\n",
    "    print(f\"Rozpoczęto run w MLflow: {run.info.run_id}\")\n",
    "    print(\"Rozpoczynam porównywanie modeli (ze stabilizacją i modelem dummy)...\")\n",
    "    \n",
    "    # Dodajemy 'dummy' do listy, aby mieć punkt odniesienia\n",
    "    models_to_try = ['lightgbm', 'xgboost', 'rf', 'dummy']\n",
    "    \n",
    "    best_model = compare_models(include=models_to_try, sort='F1')\n",
    "    \n",
    "    print(\"\\nPorównanie modeli zakończone.\")\n",
    "    \n",
    "    all_models_metrics = pull()\n",
    "    display(all_models_metrics)\n",
    "    \n",
    "    if not all_models_metrics.empty:\n",
    "        all_models_metrics.to_csv(\"compare_models_results_v2.csv\")\n",
    "        mlflow.log_artifact(\"compare_models_results_v2.csv\")\n",
    "        print(f\"\\nNajlepszy zidentyfikowany model: {best_model}\")\n",
    "    else:\n",
    "        print(\"\\nUWAGA: Żaden z testowanych modeli nie zakończył treningu pomyślnie.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d9bbc9-0324-45d8-b372-daef8684198f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
