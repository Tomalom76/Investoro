{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248bfed6-c3e8-45e1-8875-f809975715b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEKCJA 1: IMPORTY I KONFIGURACJA ===\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from pycaret.classification import setup, pull, compare_models, create_model, tune_model, plot_model, finalize_model, save_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "# Konfiguracja MLflow\n",
    "MLFLOW_EXPERIMENT_NAME = 'Investoro_District_Classification' # Nowa, bardziej adekwatna nazwa\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\") # Upewnij się, że to poprawny adres\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "print(f\"MLflow ustawiony. Eksperyment: '{MLFLOW_EXPERIMENT_NAME}'\")\n",
    "\n",
    "# Wczytanie danych\n",
    "df_original = pd.read_csv('data.csv', sep=',')\n",
    "print(f\"Wczytano dane. Kształt: {df_original.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caf91ad-ffa4-471f-abc6-c95458dd4ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original .head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca72c026-ba63-4929-8169-08c700306546",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e33c040-5722-4388-82fc-bd03c219d47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original [df_original .duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d919fc-99ae-460b-8954-ba9c4b30ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original [df_original .duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72751628-3516-4cb6-81e9-b56a0919fd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_temp = df_original.copy()\n",
    "if pd.api.types.is_string_dtype(df_corr_temp['BuiltYear']):\n",
    "    df_corr_temp['BuiltYear_Num'] = pd.to_datetime(df_corr_temp['BuiltYear'], format='%Y', errors='coerce').dt.year\n",
    "elif pd.api.types.is_datetime64_any_dtype(df_corr_temp['BuiltYear']):\n",
    "     df_corr_temp['BuiltYear_Num'] = df_corr_temp['BuiltYear'].dt.year\n",
    "else:\n",
    "    df_corr_temp['BuiltYear_Num'] = pd.to_numeric(df_corr_temp['BuiltYear'], errors='coerce') # Ostateczna próba\n",
    "\n",
    "cols_for_corr = ['Area', 'Price', 'BuiltYear_Num', 'Floor', 'Floors','Location', 'CountyNumber', 'CommunityNumber',\n",
    "                   'RegionNumber','KindNumber']\n",
    "# Upewnij się, że wszystkie kolumny istnieją i są numeryczne\n",
    "valid_cols_for_corr = [col for col in cols_for_corr if col in df_corr_temp.columns and pd.api.types.is_numeric_dtype(df_corr_temp[col])]\n",
    "correlation_matrix = df_corr_temp[valid_cols_for_corr].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57370896-3ed7-4c62-8136-db40880f225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix of Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c873e64-fbb1-435d-888e-79247f4ad400",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original .isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4341e4-de5d-4579-9dc7-3a1202b21256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEKCJA 2: CZYSZCZENIE DANYCH ===\n",
    "print(f\"Oryginalny rozmiar danych: {df_original.shape}\")\n",
    "df_cleaned = df_original.copy()\n",
    "\n",
    "# Krok 1: Usunięcie kluczowych braków danych\n",
    "df_cleaned.dropna(subset=['Area', 'Price', 'Location', 'Description'], inplace=True)\n",
    "print(f\"Rozmiar po dropna (Area, Price, Location, Description): {df_cleaned.shape}\")\n",
    "\n",
    "# Krok 2: Usunięcie outlierów na podstawie ceny (IQR)\n",
    "Q1_price = df_cleaned[\"Price\"].quantile(0.25)\n",
    "Q3_price = df_cleaned[\"Price\"].quantile(0.75)\n",
    "IQR_price = Q3_price - Q1_price\n",
    "df_cleaned = df_cleaned[~((df_cleaned[\"Price\"] < (Q1_price - 1.5 * IQR_price)) | (df_cleaned[\"Price\"] > (Q3_price + 1.5 * IQR_price)))]\n",
    "print(f\"Rozmiar po usunięciu outlierów z Price: {df_cleaned.shape}\")\n",
    "\n",
    "# Krok 3: Usunięcie outlierów na podstawie powierzchni (IQR)\n",
    "Q1_area = df_cleaned[\"Area\"].quantile(0.25)\n",
    "Q3_area = df_cleaned[\"Area\"].quantile(0.75)\n",
    "IQR_area = Q3_area - Q1_area\n",
    "df_cleaned = df_cleaned[~((df_cleaned[\"Area\"] < (Q1_area - 1.5 * IQR_area)) | (df_cleaned[\"Area\"] > (Q3_area + 1.5 * IQR_area)))]\n",
    "print(f\"Rozmiar po usunięciu outlierów z Area: {df_cleaned.shape}\")\n",
    "\n",
    "# Krok 4: Konwersja daty\n",
    "df_cleaned['BuiltYear'] = pd.to_datetime(df_cleaned['BuiltYear'], format='%Y', errors='coerce')\n",
    "\n",
    "print(\"\\nCzyszczenie danych zakończone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ce2f5c-9841-43f9-bf71-7690c6b3640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Informacje o df_cleaned po wszystkich krokach czyszczenia:\")\n",
    "df_cleaned.info()\n",
    "print(\"\\nBraki danych w df_cleaned (%):\")\n",
    "display(df_cleaned.isnull().sum() / len(df_cleaned) * 100)\n",
    "print(\"\\nPierwsze wiersze df_cleaned:\")\n",
    "display(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c6c15a-068f-4755-af23-72a1295d8245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEKCJA 3: INŻYNIERIA CECH (NA CAŁYM ZBIORZE) ===\n",
    "df_features = df_cleaned.copy()\n",
    "\n",
    "# --- Krok 3a: Ekstrakcja Dzielnicy ---\n",
    "def extract_district(location_str):\n",
    "    if not isinstance(location_str, str): return np.nan\n",
    "    parts = [part.strip() for part in location_str.split(',')]\n",
    "    if len(parts) >= 3: return f\"{parts[1]}, {parts[2]}\" # Miasto, Dzielnica\n",
    "    elif len(parts) == 2: return parts[1] # Samo miasto\n",
    "    else: return parts[0]\n",
    "\n",
    "df_features['District'] = df_features['Location'].apply(extract_district)\n",
    "df_features.dropna(subset=['District'], inplace=True)\n",
    "print(f\"Dodano kolumnę 'District'. Liczba unikalnych wartości: {df_features['District'].nunique()}\")\n",
    "\n",
    "# --- Krok 3b: Tworzenie cech TF-IDF z opisu ---\n",
    "vectorizer = TfidfVectorizer(max_features=100, ngram_range=(1, 2), min_df=5, max_df=0.95)\n",
    "\n",
    "# Używamy fit_transform na całym zbiorze, bo to unsupervised.\n",
    "# Indeksy muszą się zgadzać, dlatego resetujemy je przed i po.\n",
    "df_features.reset_index(drop=True, inplace=True)\n",
    "tfidf_features = vectorizer.fit_transform(df_features['Description'].fillna(''))\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=['des_tfidf_' + name for name in feature_names])\n",
    "\n",
    "# Łączymy wszystko w jeden finalny DataFrame\n",
    "df_final = pd.concat([df_features, tfidf_df], axis=1)\n",
    "print(f\"Dodano cechy TF-IDF. Finalny kształt zbioru do modelowania: {df_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cc48c2-464a-4435-9637-d1018ac7e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Rozmiar df_cleaned przed podziałem na train/holdout: {df_final.shape}\")\n",
    "train_df = df_final.sample(frac=0.9, random_state=42)\n",
    "holdout_df = df_final.drop(train_df.index)\n",
    "\n",
    "print(f\"Rozmiar zbioru treningowego (train_df): {train_df.shape}\")\n",
    "print(f\"Rozmiar zbioru holdout (holdout_df): {holdout_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d522c1c-57a5-4843-82ea-e5506345965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Utwórz dedykowany katalog dla tego testu, jeśli nie istnieje\n",
    "current_directory = os.getcwd() \n",
    "local_mlruns_path = os.path.join(current_directory, \"mlruns_DIRECT_LOCAL_TEST\") \n",
    "\n",
    "if not os.path.exists(local_mlruns_path):\n",
    "    os.makedirs(local_mlruns_path)\n",
    "    print(f\"Utworzono katalog: {local_mlruns_path}\")\n",
    "else:\n",
    "    print(f\"Katalog już istnieje: {local_mlruns_path}\")\n",
    "\n",
    "absolute_mlruns_path = os.path.abspath(local_mlruns_path)\n",
    "tracking_uri = f\"file:///{absolute_mlruns_path.replace(os.sep, '/')}\"\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "\n",
    "print(f\"MLflow tracking URI ustawione na: {mlflow.get_tracking_uri()}\")\n",
    "\n",
    "# MLFLOW_EXPERIMENT_NAME powinno być zdefiniowane wcześniej w Twoim notebooku\n",
    "# np. MLFLOW_EXPERIMENT_NAME = 'Investoro_Ceny'\n",
    "\n",
    "try:\n",
    "    # Sprawdź, czy eksperyment istnieje\n",
    "    experiment = mlflow.get_experiment_by_name(MLFLOW_EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        experiment_id = mlflow.create_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "        print(f\"Utworzono nowy eksperyment MLflow: '{MLFLOW_EXPERIMENT_NAME}' o ID: {experiment_id}\")\n",
    "    else:\n",
    "        experiment_id = experiment.experiment_id\n",
    "        print(f\"Znaleziono istniejący eksperyment: '{MLFLOW_EXPERIMENT_NAME}' o ID: {experiment_id}\")\n",
    "    \n",
    "    # Ustaw eksperyment jako aktywny\n",
    "    mlflow.set_experiment(experiment_name=MLFLOW_EXPERIMENT_NAME)\n",
    "    print(f\"Aktywny eksperyment MLflow ustawiony na: '{MLFLOW_EXPERIMENT_NAME}'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Błąd podczas ustawiania/tworzenia eksperymentu MLflow: {e}\")\n",
    "    import traceback\n",
    "    print(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793e419e-15e1-4333-afef-3e5198465617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Oryginalny kształt danych: {train_df_processed.shape}\")\n",
    "\n",
    "# Funkcja do ekstrakcji dzielnicy z kolumny 'Location'\n",
    "def extract_district(location_str):\n",
    "    if not isinstance(location_str, str):\n",
    "        return np.nan  # Zwróć NaN, jeśli wartość nie jest stringiem\n",
    "    \n",
    "    parts = [part.strip() for part in location_str.split(',')]\n",
    "    \n",
    "    # === Logika ekstrakcji (można ją dostosować) ===\n",
    "    # Zakładamy, że interesuje nas miasto i dzielnica, jeśli są dostępne\n",
    "    # Przykład: 'Mazowieckie, Warszawa, Mokotów' -> 'Warszawa, Mokotów'\n",
    "    \n",
    "    if len(parts) >= 3:\n",
    "        # Bierzemy miasto i dzielnicę\n",
    "        return f\"{parts[1]}, {parts[2]}\"\n",
    "    elif len(parts) == 2:\n",
    "        # Jeśli jest tylko województwo i miasto, bierzemy miasto\n",
    "        return parts[1]\n",
    "    elif len(parts) == 1:\n",
    "        # Jeśli jest tylko jeden człon, bierzemy go\n",
    "        return parts[0]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Stwórz nową kolumnę 'District' w swoim głównym DataFrame\n",
    "# Użyjemy .copy(), aby uniknąć ostrzeżeń SettingWithCopyWarning\n",
    "train_df_processed = train_df_processed.copy()\n",
    "train_df_processed['District'] = train_df_processed['Location'].apply(extract_district)\n",
    "\n",
    "# Usuń wiersze, gdzie nie udało się wyodrębnić dzielnicy\n",
    "train_df_processed.dropna(subset=['District'], inplace=True)\n",
    "\n",
    "print(f\"Kształt danych po dodaniu kolumny 'District': {train_df_processed.shape}\")\n",
    "print(\"\\nPrzykładowe wyekstrahowane dzielnice:\")\n",
    "print(train_df_processed[['Location', 'District']].head(10))\n",
    "\n",
    "print(f\"\\nLiczba unikalnych dzielnic do klasyfikacji: {train_df_processed['District'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba107b8-be19-4c25-9a3d-69b3f2407468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Ustawiamy próg, ile minimalnie razy musi wystąpić dana lokalizacja\n",
    "# 2 to absolutne minimum, aby naprawić błąd. Wartości 3-5 mogą dać stabilniejszy model.\n",
    "MIN_SAMPLES_PER_LOCATION = 3 \n",
    "\n",
    "# Policz, ile razy występuje każda unikalna lokalizacja\n",
    "location_counts = train_df_processed['Location'].value_counts()\n",
    "\n",
    "# Zidentyfikuj lokalizacje, które występują rzadziej niż nasz próg\n",
    "locations_to_remove = location_counts[location_counts < MIN_SAMPLES_PER_LOCATION].index\n",
    "\n",
    "# Stwórz nową ramkę danych, usuwając wiersze z rzadkimi lokalizacjami\n",
    "df_filtered = train_df_processed[~train_df_processed['Location'].isin(locations_to_remove)]\n",
    "\n",
    "# Wyświetl informację, ile danych zostało usuniętych\n",
    "print(f\"Oryginalna liczba wierszy: {len(train_df_processed)}\")\n",
    "print(f\"Liczba usuniętych rzadkich lokalizacji: {len(locations_to_remove)}\")\n",
    "print(f\"Liczba wierszy po odfiltrowaniu: {len(df_filtered)}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7cde24-d3c7-4bc2-a9db-a2a827b81180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Sprawdź, czy jest aktywny run i zamknij go, jeśli tak\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "    print(\"Aktywny run został zamknięty.\")\n",
    "else:\n",
    "    print(\"Nie znaleziono aktywnego runu do zamknięcia.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51fcb77-806f-4a91-a53b-ff0a00b7987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEKCJA 4: FILTROWANIE, PODZIAŁ I SETUP PYCARET ===\n",
    "\n",
    "# --- Krok 4a: Filtrowanie rzadkich dzielnic ---\n",
    "MIN_SAMPLES_PER_DISTRICT = 3\n",
    "district_counts = df_final['District'].value_counts()\n",
    "districts_to_remove = district_counts[district_counts < MIN_SAMPLES_PER_DISTRICT].index\n",
    "df_model_ready = df_final[~df_final['District'].isin(districts_to_remove)]\n",
    "print(f\"Usunięto {len(districts_to_remove)} rzadkich dzielnic. Pozostało {len(df_model_ready)} wierszy.\")\n",
    "print(f\"Liczba klas do predykcji: {df_model_ready['District'].nunique()}\")\n",
    "\n",
    "# --- Krok 4b: Podział na zbiór treningowy i testowy (holdout) ---\n",
    "data_train = df_model_ready.sample(frac=0.9, random_state=1122)\n",
    "data_test = df_model_ready.drop(data_train.index)\n",
    "print(f\"Podział danych -> Treningowe: {data_train.shape}, Testowe: {data_test.shape}\")\n",
    "\n",
    "# --- Krok 4c: Definicja list cech (teraz jest to proste i jednoznaczne) ---\n",
    "numeric_features = [c for c in data_train.columns if c.startswith('des_tfidf_')] + ['Area', 'NumberOfRooms', 'Floor', 'Floors']\n",
    "categorical_features = ['BuildingType', 'BuildingCondition', 'TypeOfMarket', 'OwnerType', 'Type', 'OfferFrom', 'VoivodeshipNumber', 'CountyNumber']\n",
    "date_features = ['BuiltYear']\n",
    "\n",
    "# Usuń z list cechy, których może nie być\n",
    "numeric_features = [f for f in numeric_features if f in data_train.columns]\n",
    "categorical_features = [f for f in categorical_features if f in data_train.columns]\n",
    "\n",
    "# --- Krok 4d: Setup PyCaret ---\n",
    "# Upewnij się, że nie ma aktywnego runu\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run(run_name=\"District_Classification_Run\") as run:\n",
    "    print(f\"\\nRozpoczynam setup PyCaret w runie MLflow: {run.info.run_id}\")\n",
    "    \n",
    "    # Lista ignorowanych cech - usuwamy wszystko, co nie jest cechą lub celem\n",
    "    ignore_features = ['Location', 'District', 'Description', 'Title', 'Price'] + [c for c in data_train.columns if c.startswith('Date') and c != 'BuiltYear']\n",
    "    \n",
    "    s = setup(\n",
    "        data=data_train,\n",
    "        test_data=data_test, # Przekazujemy zbiór testowy do PyCaret\n",
    "        target='District',\n",
    "        session_id=1122,\n",
    "        log_experiment=True, # PyCaret zaloguje do naszego aktywnego runu\n",
    "        \n",
    "        numeric_features=numeric_features,\n",
    "        categorical_features=categorical_features,\n",
    "        date_features=date_features,\n",
    "        \n",
    "        ignore_features=ignore_features,\n",
    "        \n",
    "        # Zabezpieczenia\n",
    "        html=False,\n",
    "        verbose=False\n",
    "    )\n",
    "    print(\"Setup PyCaret zakończony.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae3d34a-1edb-43f3-84a8-640b784abae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEKCJA 5: PORÓWNANIE I ANALIZA MODELI ===\n",
    "\n",
    "# Uruchom w ramach tego samego bloku 'with' z poprzedniej komórki lub osobno\n",
    "# (PyCaret zapamięta sesję)\n",
    "print(\"\\nRozpoczynam porównywanie modeli...\")\n",
    "best_model = compare_models(sort='F1', n_select=1) # n_select=1 dla pewności, że dostajemy jeden model\n",
    "print(\"\\nPorównanie modeli zakończone.\")\n",
    "\n",
    "print(\"\\nTabela z metrykami dla wszystkich modeli:\")\n",
    "all_models_metrics = pull()\n",
    "display(all_models_metrics)\n",
    "\n",
    "print(f\"\\nNajlepszy zidentyfikowany model: {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f201f7fe-2302-4034-b638-ea2c4df71c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Metryki dla najlepszego modelu:\")\n",
    "best_model_metrics = all_models_metrics.head(1)\n",
    "display(best_model_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190ab190-b9f2-4f01-89e7-6d22af109d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEKCJA 6: TUNING I FINALIZACJA ===\n",
    "\n",
    "print(\"\\nRozpoczynam tuning najlepszego modelu...\")\n",
    "tuned_model = tune_model(best_model, n_iter=25, sort='F1')\n",
    "print(\"\\nTuning zakończony. Wyniki po tuningu:\")\n",
    "display(pull())\n",
    "\n",
    "print(\"\\nFinalizowanie modelu (trening na całym zbiorze danych)...\")\n",
    "final_model = finalize_model(tuned_model)\n",
    "print(f\"Model sfinalizowany: {final_model}\")\n",
    "\n",
    "# Zapisz finalny model\n",
    "save_model(final_model, 'district_classifier_final_model')\n",
    "print(\"\\nFinalny model został zapisany do pliku.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa722452-72cd-4ebc-9c65-ace7d318243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porównaj wyniki przed i po tuningu\n",
    "print(\"\\n--- Wyniki po tuningu ---\")\n",
    "tuned_metrics = pull()\n",
    "display(tuned_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309c3c40-c8f9-4ab8-8c24-ee178b50add6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Najlepszy model przed tuningiem ---\")\n",
    "display(all_models_metrics.head(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
