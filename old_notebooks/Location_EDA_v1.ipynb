{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248bfed6-c3e8-45e1-8875-f809975715b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "from pycaret.classification import setup, pull, compare_models, plot_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.utils import resample\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np\n",
    "from scipy.stats import skewnorm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcc9893-cd43-48e5-be0c-d5295912f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_EXPERIMENT_NAME = 'Investoro_Location'\n",
    "MLFLOW_TAGS = {'data': 'Investoro_location', 'library': 'pycaret'}\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83770098-0130-451c-9db4-efa96e87128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original  = pd.read_csv('data.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caf91ad-ffa4-471f-abc6-c95458dd4ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original .head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca72c026-ba63-4929-8169-08c700306546",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e33c040-5722-4388-82fc-bd03c219d47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original [df_original .duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d919fc-99ae-460b-8954-ba9c4b30ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original [df_original .duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72751628-3516-4cb6-81e9-b56a0919fd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_temp = df_original.copy()\n",
    "if pd.api.types.is_string_dtype(df_corr_temp['BuiltYear']):\n",
    "    df_corr_temp['BuiltYear_Num'] = pd.to_datetime(df_corr_temp['BuiltYear'], format='%Y', errors='coerce').dt.year\n",
    "elif pd.api.types.is_datetime64_any_dtype(df_corr_temp['BuiltYear']):\n",
    "     df_corr_temp['BuiltYear_Num'] = df_corr_temp['BuiltYear'].dt.year\n",
    "else:\n",
    "    df_corr_temp['BuiltYear_Num'] = pd.to_numeric(df_corr_temp['BuiltYear'], errors='coerce') # Ostateczna próba\n",
    "\n",
    "cols_for_corr = ['Area', 'Price', 'BuiltYear_Num', 'Floor', 'Floors','Location', 'CountyNumber', 'CommunityNumber',\n",
    "                   'RegionNumber','KindNumber']\n",
    "# Upewnij się, że wszystkie kolumny istnieją i są numeryczne\n",
    "valid_cols_for_corr = [col for col in cols_for_corr if col in df_corr_temp.columns and pd.api.types.is_numeric_dtype(df_corr_temp[col])]\n",
    "correlation_matrix = df_corr_temp[valid_cols_for_corr].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57370896-3ed7-4c62-8136-db40880f225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix of Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c873e64-fbb1-435d-888e-79247f4ad400",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original .isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4341e4-de5d-4579-9dc7-3a1202b21256",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_original.copy()\n",
    "print(f\"Rozmiar df_cleaned przed czyszczeniem: {df_cleaned.shape}\")\n",
    "\n",
    "df_cleaned.dropna(subset=['Area', 'Price', 'Location'], inplace=True)\n",
    "print(f\"Rozmiar df_cleaned po usunięciu NaN z Area, Price, Location: {df_cleaned.shape}\")\n",
    "display(df_cleaned.isnull().sum().sort_values(ascending=False).head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f525799e-cbf5-4d7c-a831-7af32874e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_price = df_cleaned[\"Price\"].quantile(0.25)\n",
    "Q3_price = df_cleaned[\"Price\"].quantile(0.75)\n",
    "IQR_price = Q3_price - Q1_price\n",
    "lower_bound_price = Q1_price - 1.5 * IQR_price\n",
    "upper_bound_price = Q3_price + 1.5 * IQR_price\n",
    "df_cleaned = df_cleaned[~((df_cleaned[\"Price\"] < lower_bound_price) | (df_cleaned[\"Price\"] > upper_bound_price))]\n",
    "print(f\"Rozmiar df_cleaned po usunięciu outlierów z Price: {df_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5ebbdd-7ff9-46ca-8ac8-9c4d1c11497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"PricePerSquareMeter\" in df_cleaned.columns and df_cleaned[\"PricePerSquareMeter\"].isnull().sum() < len(df_cleaned) * 0.8: \n",
    "    df_cleaned.dropna(subset=['PricePerSquareMeter'], inplace=True) \n",
    "    Q1_ppsm = df_cleaned[\"PricePerSquareMeter\"].quantile(0.25)\n",
    "    Q3_ppsm = df_cleaned[\"PricePerSquareMeter\"].quantile(0.75)\n",
    "    IQR_ppsm = Q3_ppsm - Q1_ppsm\n",
    "    lower_bound_ppsm = Q1_ppsm - 1.5 * IQR_ppsm\n",
    "    upper_bound_ppsm = Q3_ppsm + 1.5 * IQR_ppsm\n",
    "    df_cleaned = df_cleaned[~((df_cleaned[\"PricePerSquareMeter\"] < lower_bound_ppsm) | (df_cleaned[\"PricePerSquareMeter\"] > upper_bound_ppsm))]\n",
    "    print(f\"Rozmiar df_cleaned po usunięciu outlierów z PricePerSquareMeter: {df_cleaned.shape}\")\n",
    "else:\n",
    "    print(\"Kolumna 'PricePerSquareMeter' nie użyta do usuwania outlierów (brak lub za dużo NaN).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003654a5-06fc-4cb3-ab40-740d2c1d59d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_area = df_cleaned[\"Area\"].quantile(0.25)\n",
    "Q3_area = df_cleaned[\"Area\"].quantile(0.75)\n",
    "IQR_area = Q3_area - Q1_area\n",
    "lower_bound_area = Q1_area - 1.5 * IQR_area\n",
    "upper_bound_area = Q3_area + 1.5 * IQR_area\n",
    "df_cleaned = df_cleaned[~((df_cleaned[\"Area\"] < lower_bound_area) | (df_cleaned[\"Area\"] > upper_bound_area))]\n",
    "print(f\"Rozmiar df_cleaned po usunięciu outlierów z Area: {df_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4758bc-9e29-4d0d-8572-b21e4301a1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['BuiltYear'] = pd.to_datetime(df_cleaned['BuiltYear'], format='%Y', errors='coerce')\n",
    "print(\"Konwersja BuiltYear na datetime w df_cleaned zakończona.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ce2f5c-9841-43f9-bf71-7690c6b3640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Informacje o df_cleaned po wszystkich krokach czyszczenia:\")\n",
    "df_cleaned.info()\n",
    "print(\"\\nBraki danych w df_cleaned (%):\")\n",
    "display(df_cleaned.isnull().sum() / len(df_cleaned) * 100)\n",
    "print(\"\\nPierwsze wiersze df_cleaned:\")\n",
    "display(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cc48c2-464a-4435-9637-d1018ac7e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Rozmiar df_cleaned przed podziałem na train/holdout: {df_cleaned.shape}\")\n",
    "train_df = df_cleaned.sample(frac=0.9, random_state=42)\n",
    "holdout_df = df_cleaned.drop(train_df.index)\n",
    "\n",
    "print(f\"Rozmiar zbioru treningowego (train_df): {train_df.shape}\")\n",
    "print(f\"Rozmiar zbioru holdout (holdout_df): {holdout_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455a7b17-48fd-45cd-8866-2eb95a43038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_column_types(df_to_convert):\n",
    "    df_copy = df_to_convert.copy()\n",
    "    str_cols = ['VoivodeshipNumber', 'CountyNumber', 'CommunityNumber', 'KindNumber', 'RegionNumber', 'StreetNumber'] # Dodano StreetNumber\n",
    "    for col in str_cols:\n",
    "        if col in df_copy.columns:\n",
    "            df_copy[col] = df_copy[col].astype(str)\n",
    "    \n",
    "    # BuiltYear powinno być już datetime, ale upewnijmy się\n",
    "    if 'BuiltYear' in df_copy.columns and not pd.api.types.is_datetime64_any_dtype(df_copy['BuiltYear']):\n",
    "         df_copy['BuiltYear'] = pd.to_datetime(df_copy['BuiltYear'], format='%Y', errors='coerce')\n",
    "    return df_copy\n",
    "\n",
    "train_df = convert_column_types(train_df)\n",
    "holdout_df = convert_column_types(holdout_df)\n",
    "\n",
    "print(\"\\\\nTypy danych w train_df po konwersji:\")\n",
    "train_df.info()\n",
    "print(\"\\\\nTypy danych w holdout_df po konwersji:\")\n",
    "holdout_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104f1261-9570-40b1-8b69-a664433aa9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "Description_vectorizer = TfidfVectorizer(\n",
    "    max_features=100, \n",
    "    stop_words=None,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5,\n",
    "    max_df=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbd9a2c-36ca-4ccd-b5c5-2855d0afd925",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Przetwarzanie TF-IDF dla zbioru treningowego...\")\n",
    "train_df_copy = train_df.copy() # Pracujemy na kopii\n",
    "train_df_copy['Description_Clean'] = train_df_copy['Description'].fillna('').astype(str)\n",
    "train_Description_tfidf_features = Description_vectorizer.fit_transform(train_df_copy['Description_Clean'])\n",
    "\n",
    "try:\n",
    "    feature_names = Description_vectorizer.get_feature_names_out()\n",
    "except AttributeError:\n",
    "    feature_names = Description_vectorizer.get_feature_names_() \n",
    "    \n",
    "train_Description_tfidf_df = pd.DataFrame(\n",
    "    train_Description_tfidf_features.toarray(),\n",
    "    columns=['des_tfidf_' + name for name in feature_names],\n",
    "    index=train_df_copy.index # Ważne, aby zachować oryginalny indeks\n",
    ")\n",
    "print(f\"Utworzono {train_Description_tfidf_df.shape[1]} cech TF-IDF dla zbioru treningowego.\")\n",
    "\n",
    "train_df_processed = pd.concat(\n",
    "    [train_df_copy.drop(columns=['Description', 'Descriptionn_Clean'], errors='ignore'), train_Description_tfidf_df], \n",
    "    axis=1\n",
    ")\n",
    "# WAŻNE: Usuń wiersze gdzie 'Price' < 20000 (lub inna wartość) dopiero PO przetworzeniu TF-IDF, \n",
    "# aby uniknąć problemów z niedopasowaniem indeksów przy konkatenacji.\n",
    "train_df_processed = train_df_processed[train_df_processed['Price'] >= 20000] \n",
    "print(f\"Rozmiar train_df_processed po usunięciu cen < 20000: {train_df_processed.shape}\")\n",
    "display(train_df_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355d0e4a-5f9b-458e-8fc7-0dd8e99eb1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Przetwarzanie TF-IDF dla zbioru holdout...\")\n",
    "holdout_df_copy = holdout_df.copy() # Pracujemy na kopii\n",
    "holdout_df_copy['Description_Clean'] = holdout_df_copy['Description'].fillna('').astype(str)\n",
    "holdout_Description_tfidf_features = Description_vectorizer.transform(holdout_df_copy['Description_Clean']) # Użyj transform\n",
    "\n",
    "holdout_Description_tfidf_df = pd.DataFrame(\n",
    "    holdout_Description_tfidf_features.toarray(),\n",
    "    columns=['des_tfidf_' + name for name in feature_names],\n",
    "    index=holdout_df_copy.index # Ważne, aby zachować oryginalny indeks\n",
    ")\n",
    "print(f\"Utworzono {holdout_Description_tfidf_df.shape[1]} cech TF-IDF dla zbioru holdout.\")\n",
    "\n",
    "holdout_df_processed = pd.concat(\n",
    "    [holdout_df_copy.drop(columns=['Description', 'Description_Clean'], errors='ignore'), holdout_Description_tfidf_df],\n",
    "    axis=1\n",
    ")\n",
    "print(f\"Rozmiar holdout_df_processed: {holdout_df_processed.shape}\")\n",
    "display(holdout_df_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d549a0-7f39-460b-89bf-f6b842151cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features_initial = [\n",
    "    'Description','Title', 'BuildingType', 'BuildingCondition', 'TypeOfMarket', 'OwnerType', 'Type', 'OfferFrom',\n",
    "    'VoivodeshipNumber', 'CountyNumber', 'CommunityNumber', 'KindNumber', 'RegionNumber'\n",
    "]\n",
    "numeric_features_initial = [\n",
    "    'Area', 'NumberOfRooms', 'Floor', 'Floors', 'CommunityScore'\n",
    "]\n",
    "date_features_initial = ['BuiltYear']\n",
    "\n",
    "categorical_features_to_use = [col for col in categorical_features_initial if col in train_df_processed.columns]\n",
    "numeric_features_to_use = [col for col in numeric_features_initial if col in train_df_processed.columns]\n",
    "# Kolumny des_tfidf_* zostaną dodane do numeric_features wewnątrz setup\n",
    "date_features_to_use = [col for col in date_features_initial if col in train_df_processed.columns]\n",
    "\n",
    "ignore_features_list_setup = [ # (...) lista jak w oryginalnym kodzie\n",
    "    'SaleId', 'OriginalId', 'PortalId', \n",
    "    'OfferPrice', 'RealPriceAfterRenovation', 'OriginalPrice',\n",
    "    'PricePerSquareMeter', 'DateAddedToDatabase', 'DateAdded',\n",
    "    'DateLastModification', 'DateLastRaises', 'NewestDate',\n",
    "    'AvailableFrom', 'Link', 'Phone', 'MainImage', 'OtherImages',\n",
    "    'NumberOfDuplicates', 'NumberOfRaises', 'NumberOfModifications',\n",
    "    'IsDuplicatePriceLower', 'IsDuplicatePrivateOwner', 'Score', 'ScorePrecision',\n",
    "    'NumberOfCommunityComments', 'NumberOfCommunityOpinions', 'Archive',\n",
    "    'SubRegionNumber', 'EncryptedId',\n",
    "    'StreetNumber' # StreetNumber jest teraz stringiem, jeśli ma za dużo wartości, można go tu dodać\n",
    "]\n",
    "ignore_features_final = [col for col in ignore_features_list_setup if col in train_df_processed.columns]\n",
    "\n",
    "print(\"--- Informacje przed PyCaret setup ---\")\n",
    "print(\"Liczba kolumn w train_df_processed:\", len(train_df_processed.columns.tolist()))\n",
    "print(\"Cechy kategoryczne:\", categorical_features_to_use)\n",
    "print(\"Cechy numeryczne (początkowe):\", numeric_features_to_use)\n",
    "print(\"Cechy daty:\", date_features_to_use)\n",
    "print(\"Ignorowane cechy:\", ignore_features_final)\n",
    "print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d522c1c-57a5-4843-82ea-e5506345965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Utwórz dedykowany katalog dla tego testu, jeśli nie istnieje\n",
    "current_directory = os.getcwd() \n",
    "local_mlruns_path = os.path.join(current_directory, \"mlruns_DIRECT_LOCAL_TEST\") \n",
    "\n",
    "if not os.path.exists(local_mlruns_path):\n",
    "    os.makedirs(local_mlruns_path)\n",
    "    print(f\"Utworzono katalog: {local_mlruns_path}\")\n",
    "else:\n",
    "    print(f\"Katalog już istnieje: {local_mlruns_path}\")\n",
    "\n",
    "absolute_mlruns_path = os.path.abspath(local_mlruns_path)\n",
    "tracking_uri = f\"file:///{absolute_mlruns_path.replace(os.sep, '/')}\"\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "\n",
    "print(f\"MLflow tracking URI ustawione na: {mlflow.get_tracking_uri()}\")\n",
    "\n",
    "# MLFLOW_EXPERIMENT_NAME powinno być zdefiniowane wcześniej w Twoim notebooku\n",
    "# np. MLFLOW_EXPERIMENT_NAME = 'Investoro_Ceny'\n",
    "\n",
    "try:\n",
    "    # Sprawdź, czy eksperyment istnieje\n",
    "    experiment = mlflow.get_experiment_by_name(MLFLOW_EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        experiment_id = mlflow.create_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "        print(f\"Utworzono nowy eksperyment MLflow: '{MLFLOW_EXPERIMENT_NAME}' o ID: {experiment_id}\")\n",
    "    else:\n",
    "        experiment_id = experiment.experiment_id\n",
    "        print(f\"Znaleziono istniejący eksperyment: '{MLFLOW_EXPERIMENT_NAME}' o ID: {experiment_id}\")\n",
    "    \n",
    "    # Ustaw eksperyment jako aktywny\n",
    "    mlflow.set_experiment(experiment_name=MLFLOW_EXPERIMENT_NAME)\n",
    "    print(f\"Aktywny eksperyment MLflow ustawiony na: '{MLFLOW_EXPERIMENT_NAME}'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Błąd podczas ustawiania/tworzenia eksperymentu MLflow: {e}\")\n",
    "    import traceback\n",
    "    print(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793e419e-15e1-4333-afef-3e5198465617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Oryginalny kształt danych: {train_df_processed.shape}\")\n",
    "\n",
    "# Funkcja do ekstrakcji dzielnicy z kolumny 'Location'\n",
    "def extract_district(location_str):\n",
    "    if not isinstance(location_str, str):\n",
    "        return np.nan  # Zwróć NaN, jeśli wartość nie jest stringiem\n",
    "    \n",
    "    parts = [part.strip() for part in location_str.split(',')]\n",
    "    \n",
    "    # === Logika ekstrakcji (można ją dostosować) ===\n",
    "    # Zakładamy, że interesuje nas miasto i dzielnica, jeśli są dostępne\n",
    "    # Przykład: 'Mazowieckie, Warszawa, Mokotów' -> 'Warszawa, Mokotów'\n",
    "    \n",
    "    if len(parts) >= 3:\n",
    "        # Bierzemy miasto i dzielnicę\n",
    "        return f\"{parts[1]}, {parts[2]}\"\n",
    "    elif len(parts) == 2:\n",
    "        # Jeśli jest tylko województwo i miasto, bierzemy miasto\n",
    "        return parts[1]\n",
    "    elif len(parts) == 1:\n",
    "        # Jeśli jest tylko jeden człon, bierzemy go\n",
    "        return parts[0]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Stwórz nową kolumnę 'District' w swoim głównym DataFrame\n",
    "# Użyjemy .copy(), aby uniknąć ostrzeżeń SettingWithCopyWarning\n",
    "train_df_processed = train_df_processed.copy()\n",
    "train_df_processed['District'] = train_df_processed['Location'].apply(extract_district)\n",
    "\n",
    "# Usuń wiersze, gdzie nie udało się wyodrębnić dzielnicy\n",
    "train_df_processed.dropna(subset=['District'], inplace=True)\n",
    "\n",
    "print(f\"Kształt danych po dodaniu kolumny 'District': {train_df_processed.shape}\")\n",
    "print(\"\\nPrzykładowe wyekstrahowane dzielnice:\")\n",
    "print(train_df_processed[['Location', 'District']].head(10))\n",
    "\n",
    "print(f\"\\nLiczba unikalnych dzielnic do klasyfikacji: {train_df_processed['District'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba107b8-be19-4c25-9a3d-69b3f2407468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Ustawiamy próg, ile minimalnie razy musi wystąpić dana lokalizacja\n",
    "# 2 to absolutne minimum, aby naprawić błąd. Wartości 3-5 mogą dać stabilniejszy model.\n",
    "MIN_SAMPLES_PER_LOCATION = 3 \n",
    "\n",
    "# Policz, ile razy występuje każda unikalna lokalizacja\n",
    "location_counts = train_df_processed['Location'].value_counts()\n",
    "\n",
    "# Zidentyfikuj lokalizacje, które występują rzadziej niż nasz próg\n",
    "locations_to_remove = location_counts[location_counts < MIN_SAMPLES_PER_LOCATION].index\n",
    "\n",
    "# Stwórz nową ramkę danych, usuwając wiersze z rzadkimi lokalizacjami\n",
    "df_filtered = train_df_processed[~train_df_processed['Location'].isin(locations_to_remove)]\n",
    "\n",
    "# Wyświetl informację, ile danych zostało usuniętych\n",
    "print(f\"Oryginalna liczba wierszy: {len(train_df_processed)}\")\n",
    "print(f\"Liczba usuniętych rzadkich lokalizacji: {len(locations_to_remove)}\")\n",
    "print(f\"Liczba wierszy po odfiltrowaniu: {len(df_filtered)}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7cde24-d3c7-4bc2-a9db-a2a827b81180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Sprawdź, czy jest aktywny run i zamknij go, jeśli tak\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "    print(\"Aktywny run został zamknięty.\")\n",
    "else:\n",
    "    print(\"Nie znaleziono aktywnego runu do zamknięcia.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee841ce8-41d0-4141-b2c4-b6f3a47da469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- KROK 2a: FILTROWANIE DANYCH WG NOWEJ KOLUMNY 'District' ---\n",
    "MIN_SAMPLES_PER_DISTRICT = 3 # Zacznij od 3, w razie błędu zwiększ\n",
    "\n",
    "district_counts = train_df_processed['District'].value_counts()\n",
    "districts_to_remove = district_counts[district_counts < MIN_SAMPLES_PER_DISTRICT].index\n",
    "df_filtered = train_df_processed[~train_df_processed['District'].isin(districts_to_remove)]\n",
    "\n",
    "print(f\"Liczba wierszy po odfiltrowaniu rzadkich dzielnic: {len(df_filtered)}\")\n",
    "print(f\"Liczba unikalnych dzielnic po filtrowaniu: {df_filtered['District'].nunique()}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- KROK 2b: SETUP Z NOWYM CELEM ---\n",
    "class_exp = None\n",
    "try:\n",
    "    # Upewnij się, że 'Location' i 'District' są na liście ignorowanych,\n",
    "    # aby uniknąć wycieku danych.\n",
    "    features_to_ignore = ignore_features_final + ['Location', 'District']\n",
    "    \n",
    "    class_exp = setup(\n",
    "        data=df_filtered,\n",
    "        target='District',  # <-- NAJWAŻNIEJSZA ZMIANA\n",
    "        \n",
    "        # Reszta parametrów\n",
    "        log_experiment=False, # Zostawiamy False, aby uniknąć błędów z logowaniem\n",
    "        experiment_name=MLFLOW_EXPERIMENT_NAME + \"_District_Classification\",\n",
    "        categorical_features=categorical_features_to_use,\n",
    "        numeric_features=numeric_features_to_use + [col for col in df_filtered.columns if 'des_tfidf_' in col],\n",
    "        date_features=date_features_to_use,\n",
    "        ignore_features=features_to_ignore, # Używamy zaktualizowanej listy\n",
    "        session_id=1122,\n",
    "        html=False,\n",
    "        verbose=False\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Błąd: {e}\")\n",
    "\n",
    "# ... a następnie komórka z compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c01c7cf-d53f-40d0-9b09-884fd9ffa3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rozpoczynam porównywanie modeli...\")\n",
    "\n",
    "# Uruchom compare_models i zapisz najlepszy model do zmiennej `best_model`\n",
    "# Możesz zmienić metrykę sortowania, np. na 'F1'\n",
    "best_model = compare_models(sort='F1') \n",
    "\n",
    "# Po zakończeniu, `pull()` da Ci tabelę z wynikami\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Tabela z metrykami dla wszystkich modeli:\")\n",
    "all_models_metrics = pull()\n",
    "display(all_models_metrics)\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Teraz masz też dostęp do samego obiektu najlepszego modelu\n",
    "print(f\"\\nNajlepszy zidentyfikowany model to: {best_model}\")\n",
    "print(\"\\nMożesz teraz używać zmiennej 'best_model' do dalszych operacji (np. tune_model, predict_model).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190ab190-b9f2-4f01-89e7-6d22af109d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f201f7fe-2302-4034-b638-ea2c4df71c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Metryki dla najlepszego modelu:\")\n",
    "best_model_metrics = all_models_metrics.head(1)\n",
    "display(best_model_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4677cc9c-007c-4f34-a280-31c86099d6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model = tune_model(best_model, n_iter=25, sort='F1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa722452-72cd-4ebc-9c65-ace7d318243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porównaj wyniki przed i po tuningu\n",
    "print(\"\\n--- Wyniki po tuningu ---\")\n",
    "tuned_metrics = pull()\n",
    "display(tuned_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309c3c40-c8f9-4ab8-8c24-ee178b50add6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Najlepszy model przed tuningiem ---\")\n",
    "display(all_models_metrics.head(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
